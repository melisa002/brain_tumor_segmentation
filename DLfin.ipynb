{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KSdam_uwJR0"
      },
      "source": [
        "# DeepLearning Term Project\n",
        "\n",
        "#### Presented by: Melisa Vadenja, Tirdod Behbehani"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdYPjVNrXTDp"
      },
      "source": [
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In this project we develop a deep learning pipeline for detecting and segmenting tumors in brain MRI scans by training a Convolutional Neural Network (CNN) to classify scans as tumor or non-tumor and use a segmentation model to indicate the different tumor regions. (Add more here)\n",
        "\n",
        "## 2. Methodology\n",
        "\n",
        "Our dataset includes ***(how many?) MRI scans with tumor presence labels and corresponding ground-truth segmentation masks. In addition to traditional segmentation, we incorporate Meta’s Segment Anything Model (SAM) to generate automated masks. We integrate Meta’s Segment Anything Model (SAM) to generate automated segmentation masks and evaluate the alignment with the provided ground-truth masks. The objective is to assess whether SAM can reliably automate the tumor segmentation process. Additionally, we analyze the impact of using SAM-generated masks as training data for the CNN instead of manually annotated ones by measuring the potential drop in classification accuracy when the CNN is trained on SAM-derived masks compared to the original ground-truth labels, determining if this segmentation could be a good alternative to manual masking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siSTjwO_Xe5r"
      },
      "source": [
        "## 3. Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install kagglehub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP0yE50dwJSF"
      },
      "source": [
        "Dont have to run twice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCsTflCCwJSH",
        "outputId": "69f8edb4-50a2-4c8b-f92f-0196ad203a32"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"awsaf49/brats20-dataset-training-validation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfMP-1-yXyL-"
      },
      "source": [
        "# 3. Dataset Description\n",
        "\n",
        "IN this project used the Brain Tumor Segmentation (BraTS) 2020 dataset which is a collection of MRI scans used for  brain tumor segmentation.\n",
        "\n",
        "According to the dataset description, it includes MRI scans from glioma patients. Each of these patients are represented in the dataset with 4 different MRI imagings named as : Native (T1), Post-contrast T1-weighted (T1ce),T2-weighted (T2), T2-FLAIR (T2 - Fluid Attenuated Inversion Recovery) and all of these are annotated by doctors in such a way that borders the tumor in its regions, represented with these labes in the dataset:\n",
        "  1. **Label 0**: Not Tumor (NT)\n",
        "  2. **Label 1**: Necrotic and non-enhancing tumor core (NCR/NET)\n",
        "  3. **Label 2**: Peritumoral edema (ED)\n",
        "  4. **Label 3**: Missing\n",
        "  5. **Label 4**: GD-enhancing tumor (ET)\n",
        "\n",
        "However no MRIs in the dataset include label 3.\n",
        "\n",
        "The way the dataset is structured is into training and testing data where the patients are represented inside of folders as folders themselves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCL-q7oXZOF8"
      },
      "source": [
        "# 4. Exploratory data analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOVAhOmDZF-U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import glob\n",
        "import PIL\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import nibabel as nib\n",
        "import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from keras.callbacks import CSVLogger\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import data\n",
        "from skimage.util import montage\n",
        "import skimage.transform as skTrans\n",
        "from skimage.transform import rotate\n",
        "from skimage.transform import resize\n",
        "from PIL import Image, ImageOps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install matplotlib nibabel scikit-image tensorflow keras scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV2LBBGZZr36"
      },
      "outputs": [],
      "source": [
        "train_path = path + \"/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfyb3mQVZrw-",
        "outputId": "5f065d0f-e9bf-49a9-f646-563417750dc2"
      },
      "outputs": [],
      "source": [
        "# the images are in nii format, we will use nibabel to read them and then convert them to numpy arrays\n",
        "flair_test_image = nib.load(train_path + \"BraTS20_Training_355/BraTS20_Training_355_flair.nii\").get_fdata()\n",
        "print(\"Shape: \", flair_test_image.shape)\n",
        "print(\"Dtype: \", flair_test_image.dtype)\n",
        "\n",
        "# WE ARE INTERESTED IN THEIR SHAPE BCS WE WANT THEM TO SHARE A SIZE AND ALSO WE WILL RESIZE THEM TO FIT THE CNN LATER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "786UmKD5wJST",
        "outputId": "a6538794-547d-4cf4-b272-8454c055aff2"
      },
      "outputs": [],
      "source": [
        "type(flair_test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed8ADO48wJST",
        "outputId": "7fa85330-0d8b-4373-cfb5-d70711f7f1b9"
      },
      "outputs": [],
      "source": [
        "flair_test_image = np.array(flair_test_image)\n",
        "type(flair_test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flYodvu6Z7ss",
        "outputId": "4a3a0dae-6024-4f4e-fe41-c4ddd2f9387c"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum pixel value: \", flair_test_image.min())\n",
        "print(\"Maximum pixel value: \", flair_test_image.max())\n",
        "# this is good to know dor rescaling the images later and bcs we  want standardized images across all 4 types of\n",
        "# MRI scans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9-rHoOdZyWt"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLz1kGx8S6zO"
      },
      "source": [
        "#### the previous code (some_image = scaler.fit_transform(some_image.reshape(-1, some_image.shape[-1])).reshape(some_image.shape)) may  have been performing incorrect normalization.\n",
        "\n",
        "It was performing row-wise normalization, which can lead to slice inconsistency. If one slice happens to have higher intensities overall than another, it will still end up with a similar range after scaling. That can disrupt global intensity relationships between slices, which can be problematic for many 3D tasks (especially in medical imaging).\n",
        "\n",
        "\n",
        "The corrected code uses global normalization - which maintains a single, consistent intensity mapping across the entire volume of the image, which is crucial for detecting and delineating tumors that may span many slices. It is preferred when you want the entire 3d image on the same intensity scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awjUpr4bZyUX"
      },
      "outputs": [],
      "source": [
        "# we flatten, then fit a scaler to it and then turn it back into array bcs we want to normalize the pixel values but retain the image\n",
        "flair_test_image = scaler.fit_transform(flair_test_image.reshape(-1, 1)).reshape(flair_test_image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cwZGVvZZySE",
        "outputId": "c938a47f-2d9a-4c15-8714-15b3653813be"
      },
      "outputs": [],
      "source": [
        "print(\"Min: \", flair_test_image.min())\n",
        "print(\"Max: \", flair_test_image.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fC7H602aHVz"
      },
      "outputs": [],
      "source": [
        "# rescaling t1\n",
        "test_image_t1 = nib.load(train_path + 'BraTS20_Training_355/BraTS20_Training_355_t1.nii').get_fdata()\n",
        "test_image_t1 = scaler.fit_transform(test_image_t1.reshape(-1, 1)).reshape(test_image_t1.shape)\n",
        "\n",
        "# rescaling t1ce\n",
        "test_image_t1ce = nib.load(train_path + 'BraTS20_Training_355/BraTS20_Training_355_t1ce.nii').get_fdata()\n",
        "test_image_t1ce = scaler.fit_transform(test_image_t1ce.reshape(-1, 1)).reshape(test_image_t1ce.shape)\n",
        "\n",
        "# rescaling t2\n",
        "test_image_t2 = nib.load(train_path + 'BraTS20_Training_355/BraTS20_Training_355_t2.nii').get_fdata()\n",
        "test_image_t2 = scaler.fit_transform(test_image_t2.reshape(-1, 1)).reshape(test_image_t2.shape)\n",
        "\n",
        "# we will not rescale the mask\n",
        "test_image_seg = nib.load(train_path + 'BraTS20_Training_355/BraTS20_Training_355_seg.nii').get_fdata()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "abQCZZPEdloc",
        "outputId": "9bb46103-7b33-4fc2-b369-542d898d94af",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "slice = 95\n",
        "\n",
        "print(\"Slice Number: \" + str(slice))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# T1\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(test_image_t1[:,:,slice], cmap='gray')\n",
        "plt.title('T1')\n",
        "\n",
        "# T1ce\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
        "plt.title('T1ce')\n",
        "\n",
        "# T2\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(test_image_t2[:,:,slice], cmap='gray')\n",
        "plt.title('T2')\n",
        "\n",
        "# Flair\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(flair_test_image[:,:,slice], cmap='gray')\n",
        "plt.title('FLAIR')\n",
        "\n",
        "# Mask\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(test_image_seg[:,:,slice])\n",
        "plt.title('Mask')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3qKmwkfdllm",
        "outputId": "defcd512-6584-44f9-dc14-29c6f9a3cb96"
      },
      "outputs": [],
      "source": [
        "# Modality shape\n",
        "print(\"Modality: \", test_image_t1.shape)\n",
        "\n",
        "# Segmentation shape\n",
        "print(\"Segmentation: \", test_image_seg.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0VwK_yvwdlhq",
        "outputId": "124e7e08-f9d7-41a4-d80d-4e1377311b8a"
      },
      "outputs": [],
      "source": [
        "slice = 95\n",
        "\n",
        "print(\"Slice number: \" + str(slice))\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(test_image_t1ce[:,:,slice], cmap='gray')\n",
        "plt.title('T1Ce - Transverse View')\n",
        "\n",
        "# T1 - Frontal View\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(rotate(test_image_t1ce[:,slice,:], 90, resize=True), cmap='gray')\n",
        "plt.title('T1Ce - Frontal View')\n",
        "\n",
        "# T1 - Sagittal View\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(rotate(test_image_t1ce[slice,:,:], 90, resize=True), cmap='gray')\n",
        "plt.title('T1Ce - Sagittal View')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "q4rlusUcdwL4",
        "outputId": "f9ef5732-4fb6-4d66-ac03-99315f54f805"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 1, 1)\n",
        "\n",
        "# montage allows us to concatenate multiple images of the same size horizontally and vertically\n",
        "plt.imshow(rotate(montage(test_image_t1ce[:,:,:]), 90, resize=True), cmap ='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "FkEkxaTEdwIo",
        "outputId": "8a50ea40-49cd-48ec-8b0e-b5b16166cd06"
      },
      "outputs": [],
      "source": [
        "# Skip 50:-50 slices since there is not much to see\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.imshow(rotate(montage(test_image_t1ce[50:-50,:,:]), 90, resize=True), cmap ='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "7_zty__vdwGr",
        "outputId": "c7162c0c-0b76-4f35-b9f3-d6e7bf98cb8c"
      },
      "outputs": [],
      "source": [
        "# Skip 50:-50 slices since there is not much to see\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 1, 1)\n",
        "plt.imshow(rotate(montage(test_image_seg[50:-50,:,:]), 90, resize=True), cmap ='gray');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "eelTB080dwED",
        "outputId": "fc757171-1db1-40cc-e810-8374d7cee66e"
      },
      "outputs": [],
      "source": [
        "# Plotting the segmantation\n",
        "cmap = matplotlib.colors.ListedColormap(['#440054', '#3b528b', '#18b880', '#e6d74f'])\n",
        "norm = matplotlib.colors.BoundaryNorm([-0.5, 0.5, 1.5, 2.5, 3.5], cmap.N)\n",
        "\n",
        "# plotting the 95th slice\n",
        "plt.imshow(test_image_seg[:,:,95], cmap=cmap, norm=norm)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "hsomyAxsdv-y",
        "outputId": "2280a9bd-973c-4430-eb5e-363e0c719552"
      },
      "outputs": [],
      "source": [
        "# Isolation of class 0\n",
        "seg_0 = test_image_seg.copy()\n",
        "seg_0[seg_0 != 0] = np.nan\n",
        "\n",
        "# Isolation of class 1\n",
        "seg_1 = test_image_seg.copy()\n",
        "seg_1[seg_1 != 1] = np.nan\n",
        "\n",
        "# Isolation of class 2\n",
        "seg_2 = test_image_seg.copy()\n",
        "seg_2[seg_2 != 2] = np.nan\n",
        "\n",
        "# Isolation of class 4\n",
        "seg_4 = test_image_seg.copy()\n",
        "seg_4[seg_4 != 4] = np.nan\n",
        "\n",
        "# Define legend\n",
        "class_names = ['class 0', 'class 1', 'class 2', 'class 4']\n",
        "legend = [plt.Rectangle((0, 0), 1, 1, color=cmap(i), label=class_names[i]) for i in range(len(class_names))]\n",
        "\n",
        "fig, ax = plt.subplots(1, 5, figsize=(20, 20))\n",
        "\n",
        "ax[0].imshow(test_image_seg[:,:, slice], cmap=cmap, norm=norm)\n",
        "ax[0].set_title('Original Segmentation')\n",
        "ax[0].legend(handles=legend, loc='lower left')\n",
        "\n",
        "ax[1].imshow(seg_0[:,:, slice], cmap=cmap, norm=norm)\n",
        "ax[1].set_title('Not Tumor (class 0)')\n",
        "\n",
        "ax[2].imshow(seg_1[:,:, slice], cmap=cmap, norm=norm)\n",
        "ax[2].set_title('Non-Enhancing Tumor (class 1)')\n",
        "\n",
        "ax[3].imshow(seg_2[:,:, slice], cmap=cmap, norm=norm)\n",
        "ax[3].set_title('Edema (class 2)')\n",
        "\n",
        "ax[4].imshow(seg_4[:,:, slice], cmap=cmap, norm=norm)\n",
        "ax[4].set_title('Enhancing Tumor (class 4)')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCaczvCSdv8j"
      },
      "outputs": [],
      "source": [
        "# lists of directories with studies\n",
        "train_and_val_directories = [f.path for f in os.scandir(train_path) if f.is_dir()]\n",
        "\n",
        "def pathListIntoIds(dirList):\n",
        "    x = []\n",
        "    for i in range(0,len(dirList)):\n",
        "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
        "    return x\n",
        "\n",
        "train_and_test_ids = pathListIntoIds(train_and_val_directories)\n",
        "\n",
        "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2)\n",
        "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu1HVc7Odv5R",
        "outputId": "beb5ec93-560b-481b-b989-6084ad5c59f0"
      },
      "outputs": [],
      "source": [
        "# Print data distribution (Train: 68%, Test: 12%, Val: 20%)\n",
        "print(f\"Train length: {len(train_ids)}\")\n",
        "print(f\"Validation length: {len(val_ids)}\")\n",
        "print(f\"Test length: {len(test_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "wsOe3-kQdv2w",
        "outputId": "1153c747-0ea6-42dc-97d6-e1464d96c4ba"
      },
      "outputs": [],
      "source": [
        "plt.bar([\"Train\",\"Valid\",\"Test\"],\n",
        "        [len(train_ids), len(val_ids), len(test_ids)],\n",
        "        align='center',\n",
        "        color=[ 'green','red', 'blue'],\n",
        "        label=[\"Train\", \"Valid\", \"Test\"]\n",
        "       )\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Data Distribution')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ZvNzUAcPei"
      },
      "outputs": [],
      "source": [
        "# Define seg-areas\n",
        "SEGMENT_CLASSES = {\n",
        "    0 : 'NOT tumor',\n",
        "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n",
        "    2 : 'EDEMA',\n",
        "    3 : 'ENHANCING' # original 4 -> converted into 3\n",
        "}\n",
        "\n",
        "# Select Slices and Image Size\n",
        "VOLUME_SLICES = 100\n",
        "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
        "IMG_SIZE=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiCXSrbWdv0C"
      },
      "outputs": [],
      "source": [
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of batches per epoch\"\"\"\n",
        "        return int(np.floor(len(self.indexes) / self.batch_size))  # Ensure this is > 0\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(Batch_ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, Batch_ids):\n",
        "\n",
        "        # Initialization with float32 dtype\n",
        "        X = np.zeros((self.batch_size * VOLUME_SLICES, *self.dim, self.n_channels), dtype=np.float32)\n",
        "        y = np.zeros((self.batch_size * VOLUME_SLICES, 240, 240), dtype=np.uint8)  # Ensure labels are int\n",
        "        Y = np.zeros((self.batch_size * VOLUME_SLICES, *self.dim, 4), dtype=np.float32)\n",
        "\n",
        "        # Generate data\n",
        "        for c, i in enumerate(Batch_ids):\n",
        "            case_path = os.path.join(train_path, i)\n",
        "\n",
        "            # Load images\n",
        "            flair = nib.load(os.path.join(case_path, f'{i}_flair.nii')).get_fdata()\n",
        "            t1ce = nib.load(os.path.join(case_path, f'{i}_t1ce.nii')).get_fdata()\n",
        "            seg = nib.load(os.path.join(case_path, f'{i}_seg.nii')).get_fdata()\n",
        "\n",
        "            # Resize and store slices\n",
        "            for j in range(VOLUME_SLICES):\n",
        "                X[j + VOLUME_SLICES * c, :, :, 0] = cv2.resize(flair[:, :, j + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "                X[j + VOLUME_SLICES * c, :, :, 1] = cv2.resize(t1ce[:, :, j + VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "                y[j + VOLUME_SLICES * c] = seg[:, :, j + VOLUME_START_AT]\n",
        "\n",
        "        # Convert segmentation labels (4 → 3)\n",
        "        y[y == 4] = 3\n",
        "\n",
        "        # One-hot encode segmentation masks\n",
        "        mask = tf.one_hot(y, 4, dtype=tf.float32)  # Ensure float32 dtype\n",
        "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE)).numpy()  # Convert to NumPy\n",
        "\n",
        "        # Normalize `X`, ensuring no division by zero\n",
        "        X_max = np.max(X)\n",
        "        if X_max > 0:\n",
        "            X /= X_max\n",
        "\n",
        "        return X, Y  # Both as NumPy arrays\n",
        "\n",
        "training_generator = DataGenerator(train_ids)\n",
        "valid_generator = DataGenerator(val_ids)\n",
        "test_generator = DataGenerator(test_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "_381Xgj1RSo4",
        "outputId": "e5739761-0773-4a98-b0f2-a6220bf1fc95"
      },
      "outputs": [],
      "source": [
        "# Define a function to display one slice and its segmentation\n",
        "def display_slice_and_segmentation(flair, t1ce, segmentation):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "    axes[0].imshow(flair, cmap='gray')\n",
        "    axes[0].set_title('Flair')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    axes[1].imshow(t1ce, cmap='gray')\n",
        "    axes[1].set_title('T1CE')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(segmentation) # Displaying segmentation\n",
        "    axes[2].set_title('Segmentation')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Retrieve the batch from the training generator\n",
        "X_batch, Y_batch = training_generator[8]\n",
        "\n",
        "# Extract Flair, T1CE, and segmentation from the batch\n",
        "flair_batch = X_batch[:, :, :, 0]\n",
        "t1ce_batch = X_batch[:, :, :, 1]\n",
        "segmentation_batch = np.argmax(Y_batch, axis=-1)  # Convert one-hot encoded to categorical\n",
        "\n",
        "# Extract the 50th slice from Flair, T1CE, and segmentation\n",
        "slice_index = 60  # Indexing starts from 0\n",
        "slice_flair = flair_batch[slice_index]\n",
        "slice_t1ce = t1ce_batch[slice_index]\n",
        "slice_segmentation = segmentation_batch[slice_index]\n",
        "\n",
        "# Display the 50th slice and its segmentation\n",
        "display_slice_and_segmentation(slice_flair, slice_t1ce, slice_segmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# CUDA Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Constants\n",
        "VOLUME_SLICES = 100\n",
        "VOLUME_START_AT = 22\n",
        "IMG_SIZE = 128\n",
        "NUM_CLASSES = 4\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class BrainTumorDataset(Dataset):\n",
        "    def __init__(self, root_dir, patient_folders, img_size=128, volume_slices=100, volume_start=22):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the dataset root directory.\n",
        "            patient_folders (list): List of patient folder names.\n",
        "            img_size (int): Image size after resizing.\n",
        "            volume_slices (int): Number of slices per MRI scan.\n",
        "            volume_start (int): Start index for slicing.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.patient_folders = patient_folders\n",
        "        self.img_size = img_size\n",
        "        self.volume_slices = volume_slices\n",
        "        self.volume_start = volume_start\n",
        "        self.total_slices = len(self.patient_folders) * self.volume_slices  # Total dataset size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_slices\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns an MRI slice with segmentation label.\n",
        "        \"\"\"\n",
        "        # Compute patient index and slice index\n",
        "        patient_idx = idx // self.volume_slices\n",
        "        slice_idx = idx % self.volume_slices\n",
        "\n",
        "        # Get patient folder path\n",
        "        patient_id = self.patient_folders[patient_idx]\n",
        "        patient_path = os.path.join(self.root_dir, patient_id)\n",
        "\n",
        "        # **Ensure the directory exists**\n",
        "        assert os.path.isdir(patient_path), f\"Directory not found: {patient_path}\"\n",
        "\n",
        "        try:\n",
        "            # Load MRI modalities & segmentation mask\n",
        "            flair_path = os.path.join(patient_path, f\"{patient_id}_flair.nii\")\n",
        "            t1ce_path = os.path.join(patient_path, f\"{patient_id}_t1ce.nii\")\n",
        "            seg_path = os.path.join(patient_path, f\"{patient_id}_seg.nii\")\n",
        "\n",
        "            flair = nib.load(flair_path).get_fdata()\n",
        "            t1ce = nib.load(t1ce_path).get_fdata()\n",
        "            seg = nib.load(seg_path).get_fdata()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading case {patient_id}: {e}\")\n",
        "            return torch.zeros((2, self.img_size, self.img_size)), torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n",
        "\n",
        "        # Select and resize slice\n",
        "        try:\n",
        "            flair_slice = cv2.resize(flair[:, :, slice_idx + self.volume_start], (self.img_size, self.img_size))\n",
        "            t1ce_slice = cv2.resize(t1ce[:, :, slice_idx + self.volume_start], (self.img_size, self.img_size))\n",
        "            seg_slice = cv2.resize(seg[:, :, slice_idx + self.volume_start], (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)\n",
        "        except:\n",
        "            print(f\"Error resizing images for patient {patient_id}, slice {slice_idx}\")\n",
        "            return torch.zeros((2, self.img_size, self.img_size)), torch.zeros((self.img_size, self.img_size), dtype=torch.long)\n",
        "\n",
        "        # Convert segmentation labels (4 → 3 for consistency)\n",
        "        seg_slice[seg_slice == 4] = 3\n",
        "\n",
        "        # Normalize input images\n",
        "        flair_slice = flair_slice / np.max(flair_slice) if np.max(flair_slice) > 0 else flair_slice\n",
        "        t1ce_slice = t1ce_slice / np.max(t1ce_slice) if np.max(t1ce_slice) > 0 else t1ce_slice\n",
        "\n",
        "        # Stack input channels (2-channel input)\n",
        "        X = np.stack([flair_slice, t1ce_slice], axis=0)\n",
        "        Y = torch.tensor(seg_slice, dtype=torch.long)  # Segmentation mask\n",
        "\n",
        "        return torch.tensor(X, dtype=torch.float32), Y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dataset root directory (string)\n",
        "train_path = \"C:/Users/Melisa/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
        "\n",
        "# Get list of patient folder names (not full paths!)\n",
        "patient_folders = [f.name for f in os.scandir(train_path) if f.is_dir()]\n",
        "\n",
        "# Split into train & validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_ids, val_ids = train_test_split(patient_folders, test_size=0.2)\n",
        "\n",
        "# Initialize datasets (pass train_path as string, not list)\n",
        "train_dataset = BrainTumorDataset(train_path, train_ids)\n",
        "val_dataset = BrainTumorDataset(train_path, val_ids)\n",
        "\n",
        "# ✅ Now, `train_dataset[0]` should work fine\n",
        "sample_img, sample_mask = train_dataset[0]\n",
        "print(\"Image shape:\", sample_img.shape)  # Expected: (2, 128, 128)\n",
        "print(\"Mask shape:\", sample_mask.shape)  # Expected: (128, 128)\n",
        "print(\"train_path:\", train_path)  # Should be a string\n",
        "\n",
        "# Get list of patient folders (not full paths, just names)\n",
        "patient_folders = [f.name for f in os.scandir(train_path) if f.is_dir()]\n",
        "print(\"Example patient folder names:\", patient_folders[:5])  # Should be a list of strings\n",
        "\n",
        "# Check if train_path is a string and patient_folders is a list\n",
        "assert isinstance(train_path, str), f\"❌ train_path is not a string: {type(train_path)}\"\n",
        "assert isinstance(patient_folders, list), f\"❌ patient_folders is not a list: {type(patient_folders)}\"\n",
        "assert isinstance(patient_folders[0], str), f\"❌ patient_folders contains non-string: {type(patient_folders[0])}\"\n",
        "\n",
        "print(\"✅ train_path is a string and patient_folders is a list of strings.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_path = \"C:/Users/Melisa/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\"\n",
        "\n",
        "# Verify type of train_path\n",
        "print(\"Type of train_path:\", type(train_path))  # Should be <class 'str'>\n",
        "\n",
        "# Get list of patient folder names (not full paths!)\n",
        "patient_folders = [f.name for f in os.scandir(train_path) if f.is_dir()]\n",
        "print(\"Type of patient_folders:\", type(patient_folders))\n",
        "print(\"Example patient folder name:\", patient_folders[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split patient_folders (list of strings) into train and validation sets.\n",
        "train_ids, val_ids = train_test_split(patient_folders, test_size=0.2)\n",
        "\n",
        "# Initialize the dataset with train_path (string) and train_ids (list of strings)\n",
        "train_dataset = BrainTumorDataset(train_path, train_ids)\n",
        "val_dataset = BrainTumorDataset(train_path, val_ids)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = ResUNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -------------------\n",
        "# 1) Residual Double-Conv Block (mimics two consecutive convs in Keras, but with a residual connection)\n",
        "# -------------------\n",
        "class ResidualDoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super(ResidualDoubleConv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=True)\n",
        "\n",
        "        # If input and output channels differ, we match dimensions with a 1x1 conv\n",
        "        self.shortcut = None\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=True)\n",
        "\n",
        "        # Optional dropout (used in the bottleneck to match Keras dropout)\n",
        "        self.dropout = nn.Dropout2d(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x if self.shortcut is None else self.shortcut(x)\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = self.conv2(out)\n",
        "        out = self.dropout(out)  # only if dropout > 0 in this block\n",
        "        out = out + shortcut\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# 2) Residual U-Net (mimicking your Keras architecture)\n",
        "# -------------------\n",
        "class ResidualUNet(nn.Module):\n",
        "    def __init__(self, in_channels=2, out_channels=4, dropout=0.3):\n",
        "        super(ResidualUNet, self).__init__()\n",
        "\n",
        "        # ----------- Encoder -----------\n",
        "        self.enc1 = ResidualDoubleConv(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc2 = ResidualDoubleConv(32, 64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc3 = ResidualDoubleConv(64, 128)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.enc4 = ResidualDoubleConv(128, 256)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # ----------- Bottleneck (with dropout) -----------\n",
        "        self.bottleneck = ResidualDoubleConv(256, 512, dropout=dropout)\n",
        "\n",
        "        # ----------- Decoder -----------\n",
        "        self.up5 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.dec5 = ResidualDoubleConv(256 + 256, 256)\n",
        "\n",
        "        self.up6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.dec6 = ResidualDoubleConv(128 + 128, 128)\n",
        "\n",
        "        self.up7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec7 = ResidualDoubleConv(64 + 64, 64)\n",
        "\n",
        "        self.up8 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec8 = ResidualDoubleConv(32 + 32, 32)\n",
        "\n",
        "        # Final output (raw logits)\n",
        "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        c1 = self.enc1(x)\n",
        "        p1 = self.pool1(c1)\n",
        "\n",
        "        c2 = self.enc2(p1)\n",
        "        p2 = self.pool2(c2)\n",
        "\n",
        "        c3 = self.enc3(p2)\n",
        "        p3 = self.pool3(c3)\n",
        "\n",
        "        c4 = self.enc4(p3)\n",
        "        p4 = self.pool4(c4)\n",
        "\n",
        "        # Bottleneck\n",
        "        bn = self.bottleneck(p4)\n",
        "\n",
        "        # Decoder\n",
        "        u5 = self.up5(bn)\n",
        "        merge5 = torch.cat([u5, c4], dim=1)\n",
        "        d5 = self.dec5(merge5)\n",
        "\n",
        "        u6 = self.up6(d5)\n",
        "        merge6 = torch.cat([u6, c3], dim=1)\n",
        "        d6 = self.dec6(merge6)\n",
        "\n",
        "        u7 = self.up7(d6)\n",
        "        merge7 = torch.cat([u7, c2], dim=1)\n",
        "        d7 = self.dec7(merge7)\n",
        "\n",
        "        u8 = self.up8(d7)\n",
        "        merge8 = torch.cat([u8, c1], dim=1)\n",
        "        d8 = self.dec8(merge8)\n",
        "\n",
        "        out = self.out_conv(d8)\n",
        "        return out  # raw logits (apply softmax outside if needed)\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# 3) Weight Initialization (to replicate Keras's default or He Normal initialization)\n",
        "# -------------------\n",
        "def init_weights_he(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "# -------------------\n",
        "# 4) Example: Create and initialize the model\n",
        "# -------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm  # or tqdm.notebook, etc.\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Setup your model & optimizer\n",
        "# -------------------------------\n",
        "model = ResidualUNet(in_channels=2, out_channels=4, dropout=0.3).to(device)\n",
        "model.apply(init_weights_he)  # Kaiming/He initialization\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Training & Validation Loop\n",
        "# -------------------------------\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    # ======== TRAINING ========\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    # Single progress bar for entire training set\n",
        "    train_pbar = tqdm(train_loader,\n",
        "                      desc=f\"Epoch {epoch}/{num_epochs} [Training]\",\n",
        "                      leave=True)  # leave=True to keep the bar at epoch end\n",
        "\n",
        "    for images, labels in train_pbar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device).long()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "        # Pixel-wise accuracy\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct = (preds == labels).sum().item()\n",
        "        total = labels.numel()\n",
        "        train_correct += correct\n",
        "        train_total += total\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # ======== VALIDATION ========\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    # Single progress bar for entire validation set\n",
        "    val_pbar = tqdm(val_loader,\n",
        "                    desc=f\"Epoch {epoch}/{num_epochs} [Validation]\",\n",
        "                    leave=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_pbar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device).long()\n",
        "\n",
        "            val_outputs = model(images)\n",
        "            val_loss_batch = criterion(val_outputs, labels)\n",
        "            val_loss += val_loss_batch.item() * images.size(0)\n",
        "\n",
        "            val_preds = torch.argmax(val_outputs, dim=1)\n",
        "            correct = (val_preds == labels).sum().item()\n",
        "            total = labels.numel()\n",
        "            val_correct += correct\n",
        "            val_total += total\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    # ======== PRINT EPOCH METRICS ========\n",
        "    print(f\"Epoch {epoch}/{num_epochs} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f},   Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    # ======== SAVE MODEL CHECKPOINT ========\n",
        "    torch.save(model.state_dict(), f\"model2_epoch_{epoch}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_img, sample_mask = train_dataset[0]\n",
        "print(\"Image shape:\", sample_img.shape)  # Expected: (2, 128, 128)\n",
        "print(\"Mask shape:\", sample_mask.shape)  # Expected: (128, 128)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for images, labels in tqdm(train_loader):\n",
        "    print(\"Batch Shape:\", images.shape, labels.shape)\n",
        "    break  # Just to check if data is being loaded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xgiRwb0aGW3"
      },
      "source": [
        "# 7. Loss Function and Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv8TcYhUcIKv"
      },
      "outputs": [],
      "source": [
        "# dice loss as defined above for 4 classes\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "    class_num = 4\n",
        "    for i in range(class_num):\n",
        "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
        "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
        "        if i == 0:\n",
        "            total_loss = loss\n",
        "        else:\n",
        "            total_loss = total_loss + loss\n",
        "    total_loss = total_loss / class_num\n",
        "    return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxBBa8V7wJSi"
      },
      "source": [
        "### Per Class Dice Coefficient Functions\n",
        "\n",
        "These functions calculate the Dice coefficient for specific tumor classes in segmentation tasks, helping to measure the model's performance in predicting different tumor regions:\n",
        "\n",
        "- **dice_coef_necrotic:** Calculates the Dice coefficient for the necrotic (dead tissue) tumor region. It computes the intersection over the sum of squares of the true and predicted values for the necrotic class.\n",
        "  \n",
        "- **dice_coef_edema:** Calculates the Dice coefficient for the edema (swelling) tumor region. It computes the intersection over the sum of squares of the true and predicted values for the edema class.\n",
        "  \n",
        "- **dice_coef_enhancing:** Calculates the Dice coefficient for the enhancing tumor region. It computes the intersection over the sum of squares of the true and predicted values for the enhancing class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iPSwYxHHEY6"
      },
      "outputs": [],
      "source": [
        "# Define per class evaluation of dice coef\n",
        "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
        "\n",
        "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
        "\n",
        "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXSm99CvwJSj"
      },
      "source": [
        "Each function includes a small constant (`epsilon`) to avoid division by zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qtj1nqp2HHVg"
      },
      "outputs": [],
      "source": [
        "# Computing Precision\n",
        "def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "# Computing Sensitivity\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "\n",
        "# Computing Specificity\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlEcJ5ttwJSj"
      },
      "source": [
        "## dont change\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqx6w02YwJSo"
      },
      "source": [
        "Here is the visual representation of our model:\n",
        "\n",
        "![U-Net%20%281%29.png](attachment:U-Net%20%281%29.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvOUUjTgwJSo"
      },
      "source": [
        "## Build and Plot the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnDtGMSvcKKs"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "IMG_SIZE = 128  # Modify based on dataset\n",
        "model = ResUNet(in_channels=2, out_channels=4).to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Example Input\n",
        "sample_input = torch.randn(1, 2, IMG_SIZE, IMG_SIZE).to(device)\n",
        "output = model(sample_input)\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(device)# Should be (1, 4, IMG_SIZE, IMG_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define device for CUDA\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize Model\n",
        "IMG_SIZE = 128  # Modify based on your dataset\n",
        "model = ResidualUNet(in_channels=2, out_channels=4).to(device)\n",
        "\n",
        "# Define Loss Function (Equivalent to Categorical Crossentropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer (Equivalent to Keras Adam Optimizer)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Model is ready and using:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9mvgEvPXcKAT",
        "outputId": "35d400fe-8ee8-47cf-b495-f779eee31f6a"
      },
      "outputs": [],
      "source": [
        "plot_model(model,\n",
        "           show_shapes = True,\n",
        "           show_dtype=False,\n",
        "           show_layer_names = True,\n",
        "           rankdir = 'TB',\n",
        "           expand_nested = False,\n",
        "           dpi = 70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiJyd960wJSo"
      },
      "source": [
        "## Set up callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgVNQytLgZ_R"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, CSVLogger\n",
        "\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.000001, verbose=1),\n",
        "\n",
        "    ModelCheckpoint(filepath='model_.{epoch:02d}-{val_loss:.6f}.weights.h5',\n",
        "                    verbose=1, save_best_only=True, save_weights_only=True),\n",
        "\n",
        "    CSVLogger('training.log', separator=',', append=False)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9mX20Esaynd"
      },
      "source": [
        "# 9. Train and Save the Model\n",
        "\n",
        "We are now ready to train our deep neural network using the `.fit()` method in Keras. We will pass our three callbacks to this method to be executed during the training process, which will last for 35 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3Yt6R0owJSp",
        "outputId": "70cb939f-ddcb-4bbf-bab4-d889ac774b6c"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(training_generator))  # Try getting one batch\n",
        "print(\"Batch Shape:\", x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS93-TMHwJSp",
        "outputId": "0fc88258-faa3-4eae-f124-42544877984a"
      },
      "outputs": [],
      "source": [
        "x, y = training_generator.__getitem__(0)  # Get first batch\n",
        "print(\"Batch Shape (X):\", x.shape)\n",
        "print(\"Batch Shape (Y):\", y.shape)\n",
        "print(\"Data Type (X):\", x.dtype)\n",
        "print(\"Data Type (Y):\", y.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5dlttCcwJSp",
        "outputId": "137eba51-0afb-4732-d80e-9129e8cbf026"
      },
      "outputs": [],
      "source": [
        "y_test = np.argmax(y, axis=-1)  # Convert back from one-hot\n",
        "print(\"Unique labels in batch Y:\", np.unique(y_test))  # Should be [0,1,2,3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c38b3a41f4b34a26a1ed1b940656bd28",
            "116370f303ad465faa2fc711bb4ab9d0",
            "7735b0d08bc744d192922c5f3d04d4c3",
            "896931e949bc465586f42f1d6de2b7ce",
            "952fe96c7feb46a6aa66b925cd403fdf",
            "04ac9ac22db044eda67118d5123efff4",
            "641af6f7b5014471ab6f24ec3a3bf925",
            "7f001f64c5ca46cbb72ab0b01f79deac",
            "a0b8e4fe85bf4b4bab85a476c9d5bee8",
            "5ef7f051eb4041b99e601bb8b0c8eaaf",
            "e02e5e2aabc24dd88d457d4edfad5d79",
            "501a85dcf0eb4d8fbbddc5c0a76d2b18",
            "6973166624254a29a88a39bb4872f85c",
            "be3b762b7c504a39b73c4e19d6611789",
            "79ae90365e32411288d308da5c495506",
            "8fed9bd56b654e8282e2051abd63e269",
            "927bba9ac70e49d4b5370c4956962bf2",
            "0f4502b973e94f83a514357d4af8d7ee",
            "a14c5c1369cd4c57a8df012ad194018f",
            "17e7802278dc4ef3b24f3987c8ae42a5",
            "d37c623946084b83a668c3371d9837a8",
            "25b90c948c8d448b80433e248852f087"
          ]
        },
        "id": "yLX3p9OCjD24",
        "outputId": "ab6ddb02-2e51-4dbc-fb30-969586aa1d71"
      },
      "outputs": [],
      "source": [
        "K.clear_session()\n",
        "from tqdm.keras import TqdmCallback  # Progress bar for Keras\n",
        "\n",
        "from tqdm.keras import TqdmCallback  # Add progress bar\n",
        "\n",
        "history = model.fit(\n",
        "    training_generator,  # Use optimized dataset\n",
        "    epochs=10,\n",
        "    steps_per_epoch=len(training_generator),\n",
        "    callbacks=[TqdmCallback(), *callbacks],\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=len(valid_generator),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af0gMPR7jzyK"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxwGwioMbBTV"
      },
      "source": [
        "# 10. Load the Trained Model\n",
        "\n",
        "We will load our trained neural network model using Keras. The `load_model` method allows us to reload the saved model, including custom metrics and loss functions we defined during training. Here’s how we do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ------------------------------\n",
        "# Multi-class Dice (average over classes)\n",
        "# ------------------------------\n",
        "def dice_coef_avg_torch(y_pred, y_true, num_classes=4, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Computes the average Dice coefficient over all classes.\n",
        "    Expects y_pred and y_true as integer-label tensors of shape (B, H, W).\n",
        "    \"\"\"\n",
        "    dice = 0.0\n",
        "    for c in range(num_classes):\n",
        "        y_pred_c = (y_pred == c).float()\n",
        "        y_true_c = (y_true == c).float()\n",
        "        intersection = torch.sum(y_pred_c * y_true_c)\n",
        "        union = torch.sum(y_pred_c) + torch.sum(y_true_c)\n",
        "        dice += (2 * intersection + epsilon) / (union + epsilon)\n",
        "    return dice / num_classes\n",
        "\n",
        "# ------------------------------\n",
        "# Multi-class Precision\n",
        "# ------------------------------\n",
        "def precision_multi_torch(y_pred, y_true, num_classes=4, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Computes average precision over all classes.\n",
        "    Expects y_pred and y_true as integer-label tensors of shape (B, H, W).\n",
        "    \"\"\"\n",
        "    # Convert to one-hot: shape (B, H, W, C) then permute to (B, C, H, W)\n",
        "    y_pred_onehot = F.one_hot(y_pred, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    y_true_onehot = F.one_hot(y_true, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    true_positives = (y_pred_onehot * y_true_onehot).sum(dim=(2, 3))\n",
        "    predicted_positives = y_pred_onehot.sum(dim=(2, 3))\n",
        "    prec = (true_positives + epsilon) / (predicted_positives + epsilon)\n",
        "    return prec.mean().item()\n",
        "\n",
        "# ------------------------------\n",
        "# Multi-class Sensitivity (Recall)\n",
        "# ------------------------------\n",
        "def sensitivity_multi_torch(y_pred, y_true, num_classes=4, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Computes average sensitivity (recall) over all classes.\n",
        "    \"\"\"\n",
        "    y_pred_onehot = F.one_hot(y_pred, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    y_true_onehot = F.one_hot(y_true, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    true_positives = (y_pred_onehot * y_true_onehot).sum(dim=(2, 3))\n",
        "    possible_positives = y_true_onehot.sum(dim=(2, 3))\n",
        "    sens = (true_positives + epsilon) / (possible_positives + epsilon)\n",
        "    return sens.mean().item()\n",
        "\n",
        "# ------------------------------\n",
        "# Multi-class Specificity\n",
        "# ------------------------------\n",
        "def specificity_multi_torch(y_pred, y_true, num_classes=4, epsilon=1e-7):\n",
        "    \"\"\"\n",
        "    Computes average specificity over all classes.\n",
        "    \"\"\"\n",
        "    y_pred_onehot = F.one_hot(y_pred, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    y_true_onehot = F.one_hot(y_true, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    # True negatives: where both y_true and y_pred are 0\n",
        "    TN = ((1 - y_true_onehot) * (1 - y_pred_onehot)).sum(dim=(2, 3))\n",
        "    FP = ((1 - y_true_onehot) * y_pred_onehot).sum(dim=(2, 3))\n",
        "    spec = (TN + epsilon) / (TN + FP + epsilon)\n",
        "    return spec.mean().item()\n",
        "\n",
        "# ------------------------------\n",
        "# Per-Class Dice Coefficient\n",
        "# ------------------------------\n",
        "def dice_coef_class_torch(y_true, y_pred, class_index, num_classes=4, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Computes the Dice coefficient for a specific class.\n",
        "    If y_true and y_pred are 3D (B, H, W) with integer labels,\n",
        "    they are one-hot encoded to (B, C, H, W).\n",
        "    \"\"\"\n",
        "    if y_true.dim() == 3:\n",
        "        y_true = F.one_hot(y_true, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "        y_pred = F.one_hot(y_pred, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "    y_true_class = y_true[:, class_index, :, :]\n",
        "    y_pred_class = y_pred[:, class_index, :, :]\n",
        "    intersection = torch.sum(y_true_class * y_pred_class)\n",
        "    denominator = torch.sum(torch.square(y_true_class)) + torch.sum(torch.square(y_pred_class))\n",
        "    dice = (2.0 * intersection + epsilon) / (denominator + epsilon)\n",
        "    return dice.item()\n",
        "\n",
        "def dice_coef_necrotic_torch(y_true, y_pred, num_classes=4, epsilon=1e-6):\n",
        "    \"\"\"Dice coefficient for necrotic tissue (class index 1).\"\"\"\n",
        "    return dice_coef_class_torch(y_true, y_pred, class_index=1, num_classes=num_classes, epsilon=epsilon)\n",
        "\n",
        "def dice_coef_edema_torch(y_true, y_pred, num_classes=4, epsilon=1e-6):\n",
        "    \"\"\"Dice coefficient for edema (class index 2).\"\"\"\n",
        "    return dice_coef_class_torch(y_true, y_pred, class_index=2, num_classes=num_classes, epsilon=epsilon)\n",
        "\n",
        "def dice_coef_enhancing_torch(y_true, y_pred, num_classes=4, epsilon=1e-6):\n",
        "    \"\"\"Dice coefficient for enhancing tissue (class index 3).\"\"\"\n",
        "    return dice_coef_class_torch(y_true, y_pred, class_index=3, num_classes=num_classes, epsilon=epsilon)\n",
        "\n",
        "# ------------------------------\n",
        "# Inference and Metric Calculation\n",
        "# ------------------------------\n",
        "\n",
        "# Load your trained model\n",
        "# Make sure that your model, val_loader, device, and metric functions (e.g., dice_coef_avg_torch, etc.) are defined\n",
        "\n",
        "# Initialize lists to store the metrics for each epoch\n",
        "epochs = []\n",
        "dice_list = []\n",
        "precision_list = []\n",
        "sensitivity_list = []\n",
        "specificity_list = []\n",
        "dice_nec_list = []\n",
        "dice_edema_list = []\n",
        "dice_enh_list = []\n",
        "\n",
        "# Loop over epochs 1 to 10\n",
        "for epoch in range(1, 11):\n",
        "    # Construct the model path based on epoch\n",
        "    model_path = f\"model2_epoch_{epoch}.pth\"\n",
        "    print(f\"Loading model from: {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize accumulators for the metrics\n",
        "    dice_sum = 0.0\n",
        "    precision_sum = 0.0\n",
        "    sensitivity_sum = 0.0\n",
        "    specificity_sum = 0.0\n",
        "    dice_necrotic_sum = 0.0\n",
        "    dice_edema_sum = 0.0\n",
        "    dice_enhancing_sum = 0.0\n",
        "    count_batches = 0\n",
        "\n",
        "    # Compute metrics over the validation set\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)         # shape: (B, in_channels, H, W)\n",
        "            labels = labels.to(device).long()    # shape: (B, H, W)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            logits = model(images)               # shape: (B, 4, H, W)\n",
        "            preds = torch.argmax(logits, dim=1)    # shape: (B, H, W)\n",
        "\n",
        "            # Compute metrics (assuming these functions are defined and work with integer labels)\n",
        "            dice_batch = dice_coef_avg_torch(preds, labels, num_classes=4)\n",
        "            prec_batch = precision_multi_torch(preds, labels, num_classes=4)\n",
        "            sens_batch = sensitivity_multi_torch(preds, labels, num_classes=4)\n",
        "            spec_batch = specificity_multi_torch(preds, labels, num_classes=4)\n",
        "            dice_nec_batch = dice_coef_necrotic_torch(preds, labels, num_classes=4)\n",
        "            dice_ede_batch = dice_coef_edema_torch(preds, labels, num_classes=4)\n",
        "            dice_enh_batch = dice_coef_enhancing_torch(preds, labels, num_classes=4)\n",
        "\n",
        "            dice_sum += dice_batch\n",
        "            precision_sum += prec_batch\n",
        "            sensitivity_sum += sens_batch\n",
        "            specificity_sum += spec_batch\n",
        "            dice_necrotic_sum += dice_nec_batch\n",
        "            dice_edema_sum += dice_ede_batch\n",
        "            dice_enhancing_sum += dice_enh_batch\n",
        "            count_batches += 1\n",
        "\n",
        "    # Calculate averaged metrics for the current epoch\n",
        "    dice_avg = dice_sum / count_batches\n",
        "    precision_avg = precision_sum / count_batches\n",
        "    sensitivity_avg = sensitivity_sum / count_batches\n",
        "    specificity_avg = specificity_sum / count_batches\n",
        "    dice_nec_avg = dice_necrotic_sum / count_batches\n",
        "    dice_ede_avg = dice_edema_sum / count_batches\n",
        "    dice_enh_avg = dice_enhancing_sum / count_batches\n",
        "\n",
        "    # Append metrics to our lists\n",
        "    epochs.append(epoch)\n",
        "    dice_list.append(dice_avg)\n",
        "    precision_list.append(precision_avg)\n",
        "    sensitivity_list.append(sensitivity_avg)\n",
        "    specificity_list.append(specificity_avg)\n",
        "    dice_nec_list.append(dice_nec_avg)\n",
        "    dice_edema_list.append(dice_ede_avg)\n",
        "    dice_enh_list.append(dice_enh_avg)\n",
        "\n",
        "    # Print the metrics for the current epoch\n",
        "    print(f\"Epoch {epoch} Metrics:\")\n",
        "    print(f\"  Dice Coeff (avg)     : {dice_avg:.4f}\")\n",
        "    print(f\"  Precision (avg)      : {precision_avg:.4f}\")\n",
        "    print(f\"  Sensitivity (avg)    : {sensitivity_avg:.4f}\")\n",
        "    print(f\"  Specificity (avg)    : {specificity_avg:.4f}\")\n",
        "    print(f\"  Dice Necrotic (avg)  : {dice_nec_avg:.4f}\")\n",
        "    print(f\"  Dice Edema (avg)     : {dice_ede_avg:.4f}\")\n",
        "    print(f\"  Dice Enhancing (avg) : {dice_enh_avg:.4f}\")\n",
        "    print(\"========================================\")\n",
        "\n",
        "# Save all the collected metrics in a dictionary for later plotting\n",
        "all_metrics = {\n",
        "    \"epoch\": epochs,\n",
        "    \"dice\": dice_list,\n",
        "    \"precision\": precision_list,\n",
        "    \"sensitivity\": sensitivity_list,\n",
        "    \"specificity\": specificity_list,\n",
        "    \"dice_necrotic\": dice_nec_list,\n",
        "    \"dice_edema\": dice_edema_list,\n",
        "    \"dice_enhancing\": dice_enh_list\n",
        "}\n",
        "\n",
        "# Optionally, save the metrics dictionary to disk for later use:\n",
        "torch.save(all_metrics, \"validation_metrics.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def convert_to_numpy(data):\n",
        "    if isinstance(data, list):\n",
        "        return [convert_to_numpy(item) for item in data]\n",
        "    elif isinstance(data, torch.Tensor):\n",
        "        # Convert a 0-dim tensor to a Python number, else to a NumPy array.\n",
        "        return data.detach().cpu().item() if data.ndim == 0 else data.detach().cpu().numpy()\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "# Convert the stored metrics to NumPy arrays/lists if they are tensors\n",
        "epochs = convert_to_numpy(all_metrics[\"epoch\"])\n",
        "dice_values = convert_to_numpy(all_metrics[\"dice\"])\n",
        "precision_values = convert_to_numpy(all_metrics[\"precision\"])\n",
        "sensitivity_values = convert_to_numpy(all_metrics[\"sensitivity\"])\n",
        "specificity_values = convert_to_numpy(all_metrics[\"specificity\"])\n",
        "dice_nec_values = convert_to_numpy(all_metrics[\"dice_necrotic\"])\n",
        "dice_edema_values = convert_to_numpy(all_metrics[\"dice_edema\"])\n",
        "dice_enh_values = convert_to_numpy(all_metrics[\"dice_enhancing\"])\n",
        "\n",
        "metrics = {\n",
        "    \"Dice Coefficient\": dice_values,\n",
        "    \"Precision\": precision_values,\n",
        "    \"Sensitivity\": sensitivity_values,\n",
        "    \"Specificity\": specificity_values,\n",
        "    \"Dice Necrotic\": dice_nec_values,\n",
        "    \"Dice Edema\": dice_edema_values,\n",
        "    \"Dice Enhancing\": dice_enh_values,\n",
        "}\n",
        "\n",
        "# Plot each metric over epochs\n",
        "for label, values in metrics.items():\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, values, marker='o')\n",
        "    plt.title(f\"{label} over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(label)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Based on the screenshot/logs you shared:\n",
        "epochs = list(range(1, 11))\n",
        "\n",
        "# Extracted from each \"Epoch X/10 | Train Loss: ..., Train Acc: ..., Val Loss: ..., Val Acc: ...\"\n",
        "train_loss = [0.0369, 0.0155, 0.0110, 0.0090, 0.0079, 0.0071, 0.0065, 0.0062, 0.0058, 0.0055]\n",
        "train_acc  = [0.9883, 0.9952, 0.9959, 0.9966, 0.9970, 0.9972, 0.9974, 0.9976, 0.9977, 0.9978]\n",
        "\n",
        "val_loss   = [0.0225, 0.0200, 0.0200, 0.0225, 0.0236, 0.0210, 0.0215, 0.0225, 0.0250, 0.0233]\n",
        "val_acc    = [0.9921, 0.9935, 0.9938, 0.9933, 0.9938, 0.9942, 0.9942, 0.9941, 0.9941, 0.9941]\n",
        "\n",
        "# --- Plot: Loss ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_loss, marker='o', label='Train Loss')\n",
        "plt.plot(epochs, val_loss, marker='o', label='Validation Loss')\n",
        "plt.title(\"Loss over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# --- Plot: Accuracy ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, train_acc, marker='o', label='Train Accuracy')\n",
        "plt.plot(epochs, val_acc, marker='o', label='Validation Accuracy')\n",
        "plt.title(\"Accuracy over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnmUPS-hbGBt"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model('my_model.keras',\n",
        "                                   custom_objects={\"accuracy\" : tf.keras.metrics.MeanIoU(num_classes=4),\n",
        "                                                   \"dice_coef\" : dice_coef,\n",
        "                                                   \"precision\" : precision,\n",
        "                                                   \"sensitivity\" :sensitivity,\n",
        "                                                   \"specificity\" :specificity,\n",
        "                                                   \"dice_coef_necrotic\" : dice_coef_necrotic,\n",
        "                                                   \"dice_coef_edema\" : dice_coef_edema,\n",
        "                                                   \"dice_coef_enhancing\" : dice_coef_enhancing\n",
        "                                                  }, compile=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJzuqus8wJSp"
      },
      "source": [
        "We specify the path to our saved model and provide a dictionary of `custom_objects` to ensure that our custom metrics and loss functions are correctly recognized by Keras. Setting `compile=False` allows us to load the model architecture and weights without compiling it immediately, giving us the flexibility to adjust compilation settings if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPiyIPVwhpyN"
      },
      "outputs": [],
      "source": [
        "history = pd.read_csv('training.log', sep=',', engine='python')\n",
        "\n",
        "hist=history\n",
        "\n",
        "acc=hist['accuracy']\n",
        "val_acc=hist['val_accuracy']\n",
        "\n",
        "epoch=range(len(acc))\n",
        "\n",
        "loss=hist['loss']\n",
        "val_loss=hist['val_loss']\n",
        "\n",
        "train_dice=hist['dice_coef']\n",
        "val_dice=hist['val_dice_coef']\n",
        "\n",
        "f,ax=plt.subplots(1,4,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n",
        "ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(epoch,loss,'b',label='Training Loss')\n",
        "ax[1].plot(epoch,val_loss,'r',label='Validation Loss')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(epoch,train_dice,'b',label='Training dice coef')\n",
        "ax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\n",
        "ax[2].legend()\n",
        "\n",
        "ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n",
        "ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n",
        "ax[3].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFGAuFxqbboR"
      },
      "source": [
        "# 12. Predict Tumor Segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbMZSFc8wJSp"
      },
      "outputs": [],
      "source": [
        "# Compile a model and load our saved weights\n",
        "\n",
        "import tensorflow as tensorflow\n",
        "IMG_SIZE = 128\n",
        "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "best_saved_model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "\n",
        "best_saved_model.compile(loss=\"categorical_crossentropy\", optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing])\n",
        "\n",
        "best_saved_model.load_weights('model_.10-0.031059.weights.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg55tKnqwJSp"
      },
      "source": [
        "This allows you to select and use the optimal model version for your specific needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbbSXs7xwJSq"
      },
      "source": [
        "---\n",
        "\n",
        "Now, let's construct a function to predict the segmentation of a patient in the test dataset. We will display the results in the axial plane, though any other plane can be chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rKIquXRGyG8"
      },
      "outputs": [],
      "source": [
        "def imageLoader(path):\n",
        "    image = nib.load(path).get_fdata()\n",
        "    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT]\n",
        "    return np.array(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NgWU9WKwJSq"
      },
      "outputs": [],
      "source": [
        "def loadSegmentation(seg_path, volume_slices=100, volume_start=0, img_size=128):\n",
        "    \"\"\"\n",
        "    Loads the segmentation volume, returns shape = (volume_slices, img_size, img_size).\n",
        "    \"\"\"\n",
        "    seg_data = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    Y = np.zeros((volume_slices, img_size, img_size), dtype=np.uint8)\n",
        "    for i in range(volume_slices):\n",
        "        slice_idx = i + volume_start\n",
        "        # Resizing with INTER_NEAREST to preserve label integrity\n",
        "        Y[i] = cv2.resize(seg_data[:, :, slice_idx], (img_size, img_size), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Convert label 4 to label 3, if needed\n",
        "    Y[Y == 4] = 3\n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_acVSmgiL_p"
      },
      "outputs": [],
      "source": [
        "def loadDataFromDir(path, list_of_files, mriType, n_images):\n",
        "    scans = []\n",
        "    masks = []\n",
        "    for i in list_of_files[:n_images]:\n",
        "        fullPath = glob.glob( i + '/*'+ mriType +'*')[0]\n",
        "        currentScanVolume = imageLoader(fullPath)\n",
        "        currentMaskVolume = imageLoader( glob.glob( i + '/*seg*')[0] )\n",
        "        # for each slice in 3D volume, find also it's mask\n",
        "        for j in range(0, currentScanVolume.shape[2]):\n",
        "            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n",
        "            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n",
        "            scans.append(scan_img[..., np.newaxis])\n",
        "            masks.append(mask_img[..., np.newaxis])\n",
        "    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwG5OI3ciL2l"
      },
      "outputs": [],
      "source": [
        "def predictByPath(case_path,case):\n",
        "    files = next(os.walk(case_path))[2]\n",
        "    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii')\n",
        "    flair=nib.load(vol_path).get_fdata()\n",
        "\n",
        "    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii')\n",
        "    ce=nib.load(vol_path).get_fdata()\n",
        "\n",
        "\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "    return model.predict(X/np.max(X), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2PTjJ1EGOwh"
      },
      "outputs": [],
      "source": [
        "def showPredictsById(case, start_slice = 50):\n",
        "    path = f\"/Users/mel/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\n",
        "    gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n",
        "    origImage = nib.load(os.path.join(path, f'BraTS20_Training_{case}_flair.nii')).get_fdata()\n",
        "    p = predictByPath(path,case)\n",
        "\n",
        "    core = p[:,:,:,1]\n",
        "    edema= p[:,:,:,2]\n",
        "    enhancing = p[:,:,:,3]\n",
        "\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1,6, figsize = (18, 50))\n",
        "\n",
        "    for i in range(6): # for each image, add brain background\n",
        "        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n",
        "\n",
        "    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
        "    axarr[0].title.set_text('Original image flair')\n",
        "    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n",
        "    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n",
        "    axarr[1].title.set_text('Ground truth')\n",
        "    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
        "    axarr[2].title.set_text('all classes predicted')\n",
        "    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n",
        "    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n",
        "    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfMi-KYfwJSq"
      },
      "source": [
        "### use this instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmRT2AkewJSq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "###############################################################################\n",
        "# Example: One-cell code replicating the logic of \"showPredictsById\" with a\n",
        "# cleaner \"predictByPath\" approach. Assumes you already have:\n",
        "#   1) A trained model in the variable 'model' (or load it inside this cell).\n",
        "#   2) Certain global parameters (VOLUME_START_AT, IMG_SIZE, etc.) defined.\n",
        "###############################################################################\n",
        "\n",
        "# 1. Global parameters & label map (adjust as needed)\n",
        "VOLUME_START_AT = 22\n",
        "IMG_SIZE        = 128\n",
        "VOLUME_SLICES   = 100\n",
        "\n",
        "SEGMENT_CLASSES = {\n",
        "    0 : 'NOT tumor',\n",
        "    1 : 'NECROTIC/CORE',\n",
        "    2 : 'EDEMA',\n",
        "    3 : 'ENHANCING'\n",
        "}\n",
        "\n",
        "# 2. Simple loader to build a 4D array for Flair + T1CE\n",
        "def imageLoader(flair_path, t1ce_path, volume_slices=100, volume_start=22, img_size=128):\n",
        "    \"\"\"\n",
        "    Loads Flair & T1CE from NIfTI, returns shape (volume_slices, img_size, img_size, 2).\n",
        "    \"\"\"\n",
        "    flair_data = nib.load(flair_path).get_fdata()\n",
        "    t1ce_data  = nib.load(t1ce_path).get_fdata()\n",
        "\n",
        "    # Create container\n",
        "    X = np.zeros((volume_slices, img_size, img_size, 2), dtype=np.float32)\n",
        "\n",
        "    for i in range(volume_slices):\n",
        "        z = volume_start + i\n",
        "        # Flair channel\n",
        "        X[i,:,:,0] = cv2.resize(flair_data[:,:,z], (img_size, img_size))\n",
        "        # T1CE channel\n",
        "        X[i,:,:,1] = cv2.resize(t1ce_data[:,:,z], (img_size, img_size))\n",
        "\n",
        "    return X\n",
        "\n",
        "# 3. Predict function that uses the loader, normalizes input, and calls 'model'\n",
        "def predictByPath(patient_path, case_number):\n",
        "    \"\"\"\n",
        "    - patient_path: folder path containing BraTS20_Training_{case_number} files\n",
        "    - case_number: e.g. \"355\"\n",
        "    Returns: predictions of shape (volume_slices, img_size, img_size, 4)\n",
        "    \"\"\"\n",
        "    flair_path = os.path.join(patient_path, f'BraTS20_Training_{case_number}_flair.nii')\n",
        "    t1ce_path  = os.path.join(patient_path, f'BraTS20_Training_{case_number}_t1ce.nii')\n",
        "\n",
        "    # Build input\n",
        "    X = imageLoader(flair_path, t1ce_path,\n",
        "                    volume_slices=VOLUME_SLICES,\n",
        "                    volume_start=VOLUME_START_AT,\n",
        "                    img_size=IMG_SIZE)\n",
        "\n",
        "    # Normalize to [0,1] if not zero\n",
        "    max_val = X.max()\n",
        "    if max_val > 0:\n",
        "        X /= max_val\n",
        "\n",
        "    # Run inference\n",
        "    preds = model.predict(X, verbose=0)  # shape: (100, 128, 128, 4)\n",
        "    return preds\n",
        "\n",
        "# 4. Show predictions similarly to \"showPredictsById\"\n",
        "def showPredictsById(case, start_slice=60):\n",
        "    \"\"\"\n",
        "    case: e.g. \"355\"\n",
        "    start_slice: which slice from the loaded 100 slices to display\n",
        "    Plots:\n",
        "       - original Flair slice\n",
        "       - ground truth\n",
        "       - \"all classes predicted\"\n",
        "       - Edema\n",
        "       - Core\n",
        "       - Enhancing\n",
        "    in subplots(1,6).\n",
        "    \"\"\"\n",
        "    # Path to the specific BraTS20_Training_{case} folder\n",
        "    path = train_path\n",
        "\n",
        "    # Load ground truth and original Flair volume for the background\n",
        "    gt_path  = os.path.join(path, f'BraTS20_Training_{case}_seg.nii')\n",
        "    flair_path = os.path.join(path, f'BraTS20_Training_{case}_flair.nii')\n",
        "\n",
        "    gt_data    = nib.load(gt_path).get_fdata()\n",
        "    orig_flair = nib.load(flair_path).get_fdata()\n",
        "\n",
        "    # Predict\n",
        "    p = predictByPath(path, case)  # shape (100, 128, 128, 4)\n",
        "    core      = p[:,:,:,1]\n",
        "    edema     = p[:,:,:,2]\n",
        "    enhancing = p[:,:,:,3]\n",
        "\n",
        "    # Prepare figure\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1, 6, figsize=(18, 50))\n",
        "\n",
        "    # background slice from original Flair\n",
        "    flair_slice = orig_flair[:, :, start_slice + VOLUME_START_AT]\n",
        "    flair_slice_resized = cv2.resize(flair_slice, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # repeated for each of the 6 subplots\n",
        "    for i in range(6):\n",
        "        axarr[i].imshow(flair_slice_resized, cmap=\"gray\", interpolation='none')\n",
        "        axarr[i].axis('off')\n",
        "\n",
        "    # Subplot 0: original flair\n",
        "    axarr[0].title.set_text('Original image (Flair)')\n",
        "\n",
        "    # Subplot 1: ground truth\n",
        "    curr_gt = gt_data[:, :, start_slice + VOLUME_START_AT]\n",
        "    curr_gt_resized = cv2.resize(curr_gt, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "    axarr[1].imshow(curr_gt_resized, cmap=\"Reds\", alpha=0.3, interpolation='none')\n",
        "    axarr[1].title.set_text('Ground truth')\n",
        "\n",
        "    # Subplot 2: \"all classes predicted\" – show channels 1..4\n",
        "    # p[start_slice] shape = (128,128,4). We'll slice out channels 1..4 => shape(128,128,3)\n",
        "    # or we just overlay them in a single colormap\n",
        "    axarr[2].imshow(p[start_slice, :, :, 1:4], cmap=\"Reds\", alpha=0.3, interpolation='none')\n",
        "    axarr[2].title.set_text('All classes predicted')\n",
        "\n",
        "    # Subplot 3: edema\n",
        "    axarr[3].imshow(edema[start_slice], cmap=\"OrRd\", alpha=0.3, interpolation='none')\n",
        "    axarr[3].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n",
        "\n",
        "    # Subplot 4: core\n",
        "    axarr[4].imshow(core[start_slice], cmap=\"OrRd\", alpha=0.3, interpolation='none')\n",
        "    axarr[4].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n",
        "\n",
        "    # Subplot 5: enhancing\n",
        "    axarr[5].imshow(enhancing[start_slice], cmap=\"OrRd\", alpha=0.3, interpolation='none')\n",
        "    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# Usage:\n",
        "#  - Make sure you have a loaded model in a variable called 'model'.\n",
        "#  - Then call: showPredictsById(\"355\")  # as an example\n",
        "###############################################################################\n",
        "\n",
        "# Example (commented out):\n",
        "model = load_model(\"my_model.keras\", compile=False)\n",
        "showPredictsById(\"355\", start_slice=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ResidualUNet(in_channels=2, out_channels=4, dropout=0.3).to(device)\n",
        "model.load_state_dict(torch.load(\"model2_epoch_10.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# -------------------------- DATA LOADER FUNCTIONS --------------------------\n",
        "def load_volume(case_path, case):\n",
        "    \"\"\"\n",
        "    Loads the Flair and T1CE volumes for a given case and returns a numpy array X\n",
        "    of shape (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2).\n",
        "    \"\"\"\n",
        "    flair_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii')\n",
        "    ce_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii')\n",
        "    flair = nib.load(flair_path).get_fdata()\n",
        "    ce = nib.load(ce_path).get_fdata()\n",
        "\n",
        "    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2), dtype=np.float32)\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j, :, :, 0] = cv2.resize(flair[:, :, j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "        X[j, :, :, 1] = cv2.resize(ce[:, :, j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "    return X\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_volume(case_path, case):\n",
        "    \"\"\"\n",
        "    Loads the input volume, normalizes it, converts it to a PyTorch tensor,\n",
        "    runs model inference, applies softmax, and returns the probabilities.\n",
        "    Output shape: (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 4)\n",
        "    \"\"\"\n",
        "    X = load_volume(case_path, case)\n",
        "    max_val = np.max(X)\n",
        "    if max_val > 0:\n",
        "        X = X / max_val\n",
        "    # Convert to tensor and rearrange dimensions: (N, H, W, C) -> (N, C, H, W)\n",
        "    X_tensor = torch.from_numpy(X).permute(0, 3, 1, 2).to(device)\n",
        "    logits = model(X_tensor)\n",
        "    probs = F.softmax(logits, dim=1)\n",
        "    # Rearrange back: (N, C, H, W) -> (N, H, W, C)\n",
        "    probs = probs.permute(0, 2, 3, 1).cpu().numpy()\n",
        "    return probs\n",
        "\n",
        "def load_segmentation(case_path, case):\n",
        "    \"\"\"\n",
        "    Loads the ground truth segmentation volume for a given case.\n",
        "    Returns an array of shape (VOLUME_SLICES, IMG_SIZE, IMG_SIZE).\n",
        "    \"\"\"\n",
        "    seg_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii')\n",
        "    seg_data = nib.load(seg_path).get_fdata()\n",
        "    Y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        Y[j] = cv2.resize(seg_data[:, :, j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "    # Remap label 4 to 3 if present\n",
        "    Y[Y == 4] = 3\n",
        "    return Y\n",
        "\n",
        "# -------------------------- COLORIZATION FUNCTION --------------------------\n",
        "def colorize_segmentation(seg_mask):\n",
        "    \"\"\"\n",
        "    Converts a discrete segmentation mask (H, W) with values {0,1,2,3} into an RGB image.\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        0: (0, 0, 0),       # background: black\n",
        "        1: (255, 0, 0),     # necrotic/core: red\n",
        "        2: (0, 255, 0),     # edema: green\n",
        "        3: (0, 0, 255)      # enhancing: blue\n",
        "    }\n",
        "    h, w = seg_mask.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for cls, color in color_map.items():\n",
        "        rgb[seg_mask == cls] = color\n",
        "    return rgb\n",
        "\n",
        "# -------------------------- VISUALIZATION FUNCTION --------------------------\n",
        "def visualize_case(case, start_slice=50):\n",
        "    \"\"\"\n",
        "    Visualizes one slice from a given case.\n",
        "    Displays six subplots:\n",
        "      0. Original Flair slice.\n",
        "      1. Ground Truth overlay.\n",
        "      2. Multi-color predicted segmentation overlay.\n",
        "      3. NECROTIC/CORE (class 1) binary mask overlay.\n",
        "      4. EDEMA (class 2) binary mask overlay.\n",
        "      5. ENHANCING (class 3) binary mask overlay.\n",
        "    \"\"\"\n",
        "    # Set your base path where the BraTS case folders reside\n",
        "    base_path = \"C:/Users/Melisa/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
        "    case_path = os.path.join(base_path, f'BraTS20_Training_{case}')\n",
        "\n",
        "    # Load original Flair volume and ground truth segmentation\n",
        "    flair_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii')\n",
        "    flair_vol = nib.load(flair_path).get_fdata()\n",
        "    gt = load_segmentation(case_path, case)\n",
        "\n",
        "    # Get predictions from the model\n",
        "    probs = predict_volume(case_path, case)  # shape: (VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 4)\n",
        "    pred_labels = np.argmax(probs, axis=-1)    # Discrete segmentation\n",
        "\n",
        "    # Select a slice (apply offset for original volumes)\n",
        "    slice_idx = start_slice\n",
        "    flair_slice = cv2.resize(flair_vol[:, :, slice_idx+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "    gt_slice = cv2.resize(gt[slice_idx], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "    pred_slice = cv2.resize(pred_labels[slice_idx], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "    # Create a multi-color overlay for the prediction\n",
        "    seg_rgb = colorize_segmentation(pred_slice)\n",
        "    flair_3ch = cv2.cvtColor(flair_slice.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "    blended = cv2.addWeighted(flair_3ch, 0.6, seg_rgb, 0.4, 0)\n",
        "\n",
        "    # Create binary masks for each tumor class (scale 0-255)\n",
        "    mask_core = ((pred_slice == 1).astype(np.uint8)) * 255   # Necrotic/Core\n",
        "    mask_edema = ((pred_slice == 2).astype(np.uint8)) * 255     # Edema\n",
        "    mask_enhancing = ((pred_slice == 3).astype(np.uint8)) * 255 # Enhancing\n",
        "\n",
        "    # Plot six subplots\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1, 6, figsize=(18, 50))\n",
        "\n",
        "    # Subplot 0: Original Flair\n",
        "    axarr[0].imshow(flair_slice, cmap=\"gray\", interpolation='none')\n",
        "    axarr[0].set_title(\"Original Flair\")\n",
        "    axarr[0].axis('off')\n",
        "\n",
        "    # Subplot 1: Ground Truth overlay (using red overlay)\n",
        "    axarr[1].imshow(flair_slice, cmap=\"gray\", interpolation='none')\n",
        "    axarr[1].imshow(gt_slice, cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
        "    axarr[1].set_title(\"Ground Truth\")\n",
        "    axarr[1].axis('off')\n",
        "\n",
        "    # Subplot 2: Multi-color predicted segmentation overlay\n",
        "    axarr[2].imshow(blended[..., ::-1])  # Convert BGR to RGB for matplotlib\n",
        "    axarr[2].set_title(\"All Classes Predicted\")\n",
        "    axarr[2].axis('off')\n",
        "\n",
        "    # Subplot 3: NECROTIC/CORE predicted overlay\n",
        "    axarr[3].imshow(flair_slice, cmap=\"gray\", interpolation='none')\n",
        "    axarr[3].imshow(mask_core, cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
        "    axarr[3].set_title(\"Necrotic/Core Predicted\")\n",
        "    axarr[3].axis('off')\n",
        "\n",
        "    # Subplot 4: EDEMA predicted overlay\n",
        "    axarr[4].imshow(flair_slice, cmap=\"gray\", interpolation='none')\n",
        "    axarr[4].imshow(mask_edema, cmap=\"Greens\", interpolation='none', alpha=0.3)\n",
        "    axarr[4].set_title(\"Edema Predicted\")\n",
        "    axarr[4].axis('off')\n",
        "\n",
        "    # Subplot 5: ENHANCING predicted overlay\n",
        "    axarr[5].imshow(flair_slice, cmap=\"gray\", interpolation='none')\n",
        "    axarr[5].imshow(mask_enhancing, cmap=\"Blues\", interpolation='none', alpha=0.3)\n",
        "    axarr[5].set_title(\"Enhancing Predicted\")\n",
        "    axarr[5].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# -------------------------- USAGE EXAMPLE --------------------------\n",
        "# Adjust the case number as needed, for example \"355\":\n",
        "visualize_case(\"060\", start_slice=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################################\n",
        "# GLOBAL SETTINGS\n",
        "###############################################################################\n",
        "# Model expects 128x128 inputs, so we must resize each slice to 128x128 for inference\n",
        "MODEL_IMG_SIZE = 128\n",
        "VOLUME_START_AT = 22  # offset into the volume if needed\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 4-class segmentation: {0: background, 1: core, 2: edema, 3: enhancing}\n",
        "SEGMENT_CLASSES = {\n",
        "    0: 'NOT tumor',\n",
        "    1: 'NECROTIC/CORE',\n",
        "    2: 'EDEMA',\n",
        "    3: 'ENHANCING'\n",
        "}\n",
        "\n",
        "\n",
        "model = ResidualUNet(in_channels=2, out_channels=4, dropout=0.3).to(device)\n",
        "model.load_state_dict(torch.load(\"model2_epoch_10.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "###############################################################################\n",
        "# HELPER: colorize_segmentation for multi-color overlay\n",
        "###############################################################################\n",
        "def colorize_segmentation(seg_mask):\n",
        "    \"\"\"\n",
        "    seg_mask: 2D array with values in {0,1,2,3}.\n",
        "    Returns an RGB image (H, W, 3) with each class in a distinct color.\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        0: (0, 0, 0),       # background: black\n",
        "        1: (255, 0, 0),     # necrotic/core: red\n",
        "        2: (0, 255, 0),     # edema: green\n",
        "        3: (0, 0, 255)      # enhancing: blue\n",
        "    }\n",
        "    h, w = seg_mask.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for cls, (r, g, b) in color_map.items():\n",
        "        rgb[seg_mask == cls] = (r, g, b)\n",
        "    return rgb\n",
        "\n",
        "###############################################################################\n",
        "# HELPER: predict_slice\n",
        "###############################################################################\n",
        "@torch.no_grad()\n",
        "def predict_slice(flair_slice, t1ce_slice):\n",
        "    \"\"\"\n",
        "    1) Resizes each channel to MODEL_IMG_SIZE (128x128).\n",
        "    2) Normalizes (divides by max).\n",
        "    3) Feeds the slice to the model, applies softmax + argmax.\n",
        "    4) Upscales the result back to the original resolution (nearest-neighbor).\n",
        "    \"\"\"\n",
        "    original_h, original_w = flair_slice.shape\n",
        "\n",
        "    # Resize to 128x128\n",
        "    flair_resized = cv2.resize(flair_slice, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "    t1ce_resized  = cv2.resize(t1ce_slice,  (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "\n",
        "    # Stack => shape (1,2,128,128), then normalize\n",
        "    X = np.stack([flair_resized, t1ce_resized], axis=0)[np.newaxis, ...]\n",
        "    max_val = X.max()\n",
        "    if max_val > 0:\n",
        "        X /= max_val\n",
        "\n",
        "    # Inference\n",
        "    X_tensor = torch.from_numpy(X).float().to(device)\n",
        "    logits   = model(X_tensor)                  # (1,4,128,128)\n",
        "    probs    = F.softmax(logits, dim=1)         # (1,4,128,128)\n",
        "    pred     = torch.argmax(probs, dim=1)[0]    # (128,128)\n",
        "\n",
        "    # Upscale to original shape\n",
        "    pred_up = cv2.resize(pred.cpu().numpy().astype(np.uint8),\n",
        "                         (original_w, original_h),\n",
        "                         interpolation=cv2.INTER_NEAREST)\n",
        "    return pred_up\n",
        "\n",
        "###############################################################################\n",
        "# MAIN FUNCTION: visualize_case\n",
        "###############################################################################\n",
        "def visualize_case(case=\"355\", slice_idx=60):\n",
        "    \"\"\"\n",
        "    1) Loads Flair, T1CE, and seg at original resolution.\n",
        "    2) Predicts a single slice (resized to 128x128, then upsampled).\n",
        "    3) Creates multi-color overlay & separate overlays.\n",
        "    \"\"\"\n",
        "    base_path = \"C:/Users/Melisa/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"\n",
        "    case_path = os.path.join(base_path, f\"BraTS20_Training_{case}\")\n",
        "\n",
        "    # Load volumes\n",
        "    flair_vol = nib.load(os.path.join(case_path, f\"BraTS20_Training_{case}_flair.nii\")).get_fdata()\n",
        "    t1ce_vol  = nib.load(os.path.join(case_path, f\"BraTS20_Training_{case}_t1ce.nii\")).get_fdata()\n",
        "    seg_vol   = nib.load(os.path.join(case_path, f\"BraTS20_Training_{case}_seg.nii\")).get_fdata()\n",
        "\n",
        "    # Adjust label 4 -> 3 if needed\n",
        "    seg_vol[seg_vol == 4] = 3\n",
        "\n",
        "    # Pick the slice\n",
        "    z = slice_idx + VOLUME_START_AT\n",
        "    flair_slice = flair_vol[:, :, z]\n",
        "    t1ce_slice  = t1ce_vol[:, :, z]\n",
        "    seg_slice   = seg_vol[:, :, z]\n",
        "\n",
        "    # Predict\n",
        "    pred_mask = predict_slice(flair_slice, t1ce_slice)\n",
        "\n",
        "    # Build multi-color overlay\n",
        "    seg_rgb   = colorize_segmentation(pred_mask)\n",
        "    flair_3ch = cv2.cvtColor(flair_slice.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "    blended   = cv2.addWeighted(flair_3ch, 0.6, seg_rgb, 0.4, 0)\n",
        "\n",
        "    # Build separate binary masks for each class\n",
        "    mask_core       = (pred_mask == 1).astype(np.uint8)*255\n",
        "    mask_edema      = (pred_mask == 2).astype(np.uint8)*255\n",
        "    mask_enhancing  = (pred_mask == 3).astype(np.uint8)*255\n",
        "\n",
        "    # PLOTTING\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1, 6, figsize=(18, 50))\n",
        "\n",
        "    # 0) Original Flair\n",
        "    axarr[0].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[0].set_title(\"Original image flair\")\n",
        "    axarr[0].axis('off')\n",
        "\n",
        "    # 1) Ground Truth\n",
        "    axarr[1].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[1].imshow(seg_slice, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[1].set_title(\"Ground truth\")\n",
        "    axarr[1].axis('off')\n",
        "\n",
        "    # 2) All classes predicted (multi-color overlay)\n",
        "    axarr[2].imshow(blended[..., ::-1])  # BGR->RGB\n",
        "    axarr[2].set_title(\"all classes predicted\")\n",
        "    axarr[2].axis('off')\n",
        "\n",
        "    # 3) Necrotic/Core predicted\n",
        "    axarr[3].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[3].imshow(mask_core, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[3].set_title(\"NECROTIC/CORE predicted\")\n",
        "    axarr[3].axis('off')\n",
        "\n",
        "    # 4) Edema predicted\n",
        "    axarr[4].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[4].imshow(mask_edema, cmap=\"Greens\", alpha=0.4)\n",
        "    axarr[4].set_title(\"EDEMA predicted\")\n",
        "    axarr[4].axis('off')\n",
        "\n",
        "    # 5) Enhancing predicted\n",
        "    axarr[5].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[5].imshow(mask_enhancing, cmap=\"Blues\", alpha=0.4)\n",
        "    axarr[5].set_title(\"ENHANCING predicted\")\n",
        "    axarr[5].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# USAGE EXAMPLE\n",
        "###############################################################################\n",
        "# 1) Adjust 'base_path' inside visualize_case to match your environment.\n",
        "# 2) Then call:\n",
        "visualize_case(case=\"355\", slice_idx=70)\n",
        "#    or pick a slice index with a nice, large tumor region.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "probs = predict_volume(r\"C:\\Users\\Melisa\\.cache\\kagglehub\\datasets\\awsaf49\\brats20-dataset-training-validation\\versions\\1\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_350\", \"350\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slice_idx = 50  # Must be between 0 and 99\n",
        "slice_probs = probs[slice_idx]  # Now slice_probs is of shape (IMG_SIZE, IMG_SIZE, 4)\n",
        "y, x = 100, 100  # Coordinates within the 128x128 slice (since IMG_SIZE=128)\n",
        "print(\"Background probability:\", slice_probs[y, x, 0])\n",
        "print(\"Core probability:\", slice_probs[y, x, 1])\n",
        "print(\"Edema probability:\", slice_probs[y, x, 2])\n",
        "print(\"Enhancing probability:\", slice_probs[y, x, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slice_probs = probs[50]  # for slice index 50 (assuming IMG_SIZE is 128)\n",
        "region = slice_probs[90:110, 90:110, :]  # a 20x20 region\n",
        "max_probs = region.reshape(-1, region.shape[-1]).max(axis=0)\n",
        "print(\"Max probabilities in region:\", max_probs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "slice_probs = probs[50]  # choose slice index 50 for instance\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i, label in enumerate([\"Background\", \"Core\", \"Edema\", \"Enhancing\"]):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    plt.imshow(slice_probs[:, :, i], cmap='hot')\n",
        "    plt.title(label)\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################################\n",
        "# GLOBAL SETTINGS\n",
        "###############################################################################\n",
        "VOLUME_START_AT = 22     # offset into the volume\n",
        "MODEL_IMG_SIZE  = 128    # model input size (if your U-Net is trained on 128×128)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEGMENT_CLASSES = {\n",
        "    0: 'NOT tumor',\n",
        "    1: 'NECROTIC/CORE',\n",
        "    2: 'EDEMA',\n",
        "    3: 'ENHANCING'\n",
        "}\n",
        "\n",
        "# Instantiate and load your model\n",
        "model = ResidualUNet(in_channels=2, out_channels=4, dropout=0.3).to(device)\n",
        "model.load_state_dict(torch.load(\"model2_epoch_10.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "###############################################################################\n",
        "# HELPERS\n",
        "###############################################################################\n",
        "def colorize_segmentation(seg_mask):\n",
        "    \"\"\"\n",
        "    seg_mask: 2D array with {0,1,2,3}\n",
        "    Returns an RGB image (H, W, 3) with each class a distinct color.\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        0: (0, 0, 0),       # background -> black\n",
        "        1: (255, 0, 0),     # core -> red\n",
        "        2: (0, 255, 0),     # edema -> green\n",
        "        3: (0, 0, 255)      # enhancing -> blue\n",
        "    }\n",
        "    h, w = seg_mask.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for cls, (r, g, b) in color_map.items():\n",
        "        rgb[seg_mask == cls] = (r, g, b)\n",
        "    return rgb\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_slice(flair_slice, t1ce_slice):\n",
        "    \"\"\"\n",
        "    1) Resizes to MODEL_IMG_SIZE for the model,\n",
        "    2) Runs inference (softmax -> argmax),\n",
        "    3) Upscales predicted mask back to the original slice shape (nearest-neighbor).\n",
        "    \"\"\"\n",
        "    orig_h, orig_w = flair_slice.shape\n",
        "    # Resize both channels to 128×128 for the model\n",
        "    flair_resized = cv2.resize(flair_slice, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "    ce_resized    = cv2.resize(t1ce_slice,  (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "\n",
        "    # Stack into shape (1,2,128,128), normalize\n",
        "    X = np.stack([flair_resized, ce_resized], axis=0)[np.newaxis, ...]\n",
        "    max_val = X.max()\n",
        "    if max_val > 0:\n",
        "        X /= max_val\n",
        "\n",
        "    X_tensor = torch.from_numpy(X).float().to(device)  # (1,2,128,128)\n",
        "    logits   = model(X_tensor)                         # (1,4,128,128)\n",
        "    probs    = F.softmax(logits, dim=1)               # (1,4,128,128)\n",
        "    pred     = torch.argmax(probs, dim=1).cpu().numpy()  # (1,128,128)\n",
        "    pred_2d  = pred[0]                                # (128,128)\n",
        "\n",
        "    # Upscale back to original shape\n",
        "    pred_original = cv2.resize(\n",
        "        pred_2d.astype(np.uint8),\n",
        "        (orig_w, orig_h),\n",
        "        interpolation=cv2.INTER_NEAREST\n",
        "    )\n",
        "    return pred_original\n",
        "\n",
        "###############################################################################\n",
        "# MAIN VISUALIZATION\n",
        "###############################################################################\n",
        "def visualize_case(case=\"355\", slice_idx=60):\n",
        "    \"\"\"\n",
        "    1) Loads Flair/T1CE/Seg at original resolution.\n",
        "    2) Predicts the slice in 128×128, upscales back.\n",
        "    3) Overlays multi-color mask with transparent background on the Flair slice.\n",
        "    4) Also shows each class individually.\n",
        "    \"\"\"\n",
        "    base_path = \"C:/Users/Melisa/.cache/kagglehub/datasets/awsaf49/brats20-dataset-training-validation/versions/1/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData\"  # <-- Adjust\n",
        "    case_path = os.path.join(base_path, f\"BraTS20_Training_{case}\")\n",
        "\n",
        "    # Load volumes\n",
        "    flair_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii')\n",
        "    ce_path    = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii')\n",
        "    seg_path   = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii')\n",
        "\n",
        "    flair_vol = nib.load(flair_path).get_fdata()\n",
        "    ce_vol    = nib.load(ce_path).get_fdata()\n",
        "    seg_vol   = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    # Slice selection (plus offset)\n",
        "    z = slice_idx + VOLUME_START_AT\n",
        "    flair_slice = flair_vol[:, :, z]\n",
        "    ce_slice    = ce_vol[:, :, z]\n",
        "    seg_slice   = seg_vol[:, :, z]\n",
        "    # Convert label 4 -> 3 if needed\n",
        "    seg_slice[seg_slice == 4] = 3\n",
        "\n",
        "    # Predict segmentation for this slice\n",
        "    pred_mask = predict_slice(flair_slice, ce_slice)  # shape ~ (240,240)\n",
        "\n",
        "    # Create a color overlay with transparent background for class=0\n",
        "    color_seg = colorize_segmentation(pred_mask)\n",
        "    alpha_map = np.zeros_like(pred_mask, dtype=np.float32)\n",
        "    alpha_map[pred_mask > 0] = 0.6  # Show color where class != 0\n",
        "\n",
        "    # Convert flair to 3 channels for overlay\n",
        "    flair_3ch = cv2.cvtColor(flair_slice.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # We'll overlay in matplotlib using the alpha_map\n",
        "    # But for the individual classes, let's build separate masks\n",
        "    mask_core       = ((pred_mask == 1).astype(np.uint8) * 255)\n",
        "    mask_edema      = ((pred_mask == 2).astype(np.uint8) * 255)\n",
        "    mask_enhancing  = ((pred_mask == 3).astype(np.uint8) * 255)\n",
        "\n",
        "    # Plot subplots\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1, 6, figsize=(18, 50))\n",
        "\n",
        "    # 0. Original Flair\n",
        "    axarr[0].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[0].set_title(\"Original Flair\")\n",
        "    axarr[0].axis(\"off\")\n",
        "\n",
        "    # 1. Ground Truth\n",
        "    axarr[1].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[1].imshow(seg_slice, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[1].set_title(\"Ground Truth\")\n",
        "    axarr[1].axis(\"off\")\n",
        "\n",
        "    # 2. All classes predicted (multi-color with transparent background)\n",
        "    # Show flair in grayscale, then color overlay\n",
        "    axarr[2].imshow(flair_slice, cmap=\"gray\")\n",
        "    # Convert color_seg (BGR) to RGB for matplotlib\n",
        "    color_seg_rgb = color_seg[..., ::-1]  # if we want to treat color_seg as BGR\n",
        "    # But color_seg is actually RGB in the function. If you see weird colors, swap.\n",
        "    # We'll assume color_seg is already RGB from colorize_segmentation.\n",
        "    # We'll build a per-pixel alpha array\n",
        "    axarr[2].imshow(color_seg, alpha=alpha_map)\n",
        "    axarr[2].set_title(\"All Classes Predicted\")\n",
        "    axarr[2].axis(\"off\")\n",
        "\n",
        "    # 3. Necrotic/Core\n",
        "    axarr[3].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[3].imshow(mask_core, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[3].set_title(\"Necrotic/Core\")\n",
        "    axarr[3].axis(\"off\")\n",
        "\n",
        "    # 4. Edema\n",
        "    axarr[4].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[4].imshow(mask_edema, cmap=\"Greens\", alpha=0.4)\n",
        "    axarr[4].set_title(\"Edema\")\n",
        "    axarr[4].axis(\"off\")\n",
        "\n",
        "    # 5. Enhancing\n",
        "    axarr[5].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[5].imshow(mask_enhancing, cmap=\"Blues\", alpha=0.4)\n",
        "    axarr[5].set_title(\"Enhancing\")\n",
        "    axarr[5].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# USAGE EXAMPLE:\n",
        "# 1) Update 'base_path' in visualize_case to point to your actual data folder.\n",
        "# 2) Then call:\n",
        "visualize_case(case=\"350\", slice_idx=60)\n",
        "###############################################################################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "###############################################################################\n",
        "# 1) GLOBAL SETTINGS\n",
        "###############################################################################\n",
        "MODEL_IMG_SIZE  = 128   # Model was trained on 128x128\n",
        "VOLUME_START_AT = 22    # If your volumes start at slice index 22\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEGMENT_CLASSES = {\n",
        "    0: 'NOT tumor',\n",
        "    1: 'NECROTIC/CORE',\n",
        "    2: 'EDEMA',\n",
        "    3: 'ENHANCING'\n",
        "}\n",
        "\n",
        "###############################################################################\n",
        "# 2) COLOR & ALPHA FUNCTIONS\n",
        "###############################################################################\n",
        "def colorize_segmentation(seg_mask):\n",
        "    \"\"\"\n",
        "    seg_mask: 2D array with values in {0,1,2,3}.\n",
        "    Returns (H, W, 3) with each class in a distinct color:\n",
        "      0 -> black\n",
        "      1 -> red\n",
        "      2 -> green\n",
        "      3 -> blue\n",
        "    \"\"\"\n",
        "    color_map = {\n",
        "        0: (0,   0,   0),    # background\n",
        "        1: (255, 0,   0),    # core\n",
        "        2: (0,   255, 0),    # edema\n",
        "        3: (0,   0,   255)   # enhancing\n",
        "    }\n",
        "    h, w = seg_mask.shape\n",
        "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
        "    for cls, (r, g, b) in color_map.items():\n",
        "        rgb[seg_mask == cls] = (r, g, b)\n",
        "    return rgb\n",
        "\n",
        "def make_alpha_map(seg_mask, alpha_value=0.4):\n",
        "    \"\"\"\n",
        "    seg_mask: 2D array in {0,1,2,3}\n",
        "    alpha_value: float in [0,1], e.g. 0.4\n",
        "    Returns a 2D array of shape (H,W) with 0 for background, alpha_value for tumor.\n",
        "    \"\"\"\n",
        "    alpha = np.zeros_like(seg_mask, dtype=np.float32)\n",
        "    alpha[seg_mask > 0] = alpha_value\n",
        "    return alpha\n",
        "\n",
        "\n",
        "model = ResidualUNet(in_channels=2, out_channels=4, dropout=0.3).to(device)\n",
        "model.load_state_dict(torch.load(\"model2_epoch_10.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "###############################################################################\n",
        "# 4) PREDICTION FOR ONE SLICE\n",
        "###############################################################################\n",
        "@torch.no_grad()\n",
        "def predict_slice(flair_slice, t1ce_slice):\n",
        "    \"\"\"\n",
        "    1. Resize to MODEL_IMG_SIZE x MODEL_IMG_SIZE.\n",
        "    2. Normalize by max intensity.\n",
        "    3. Model inference -> softmax -> argmax.\n",
        "    4. Upscale back to original resolution with nearest-neighbor.\n",
        "    Returns a 2D array (H,W) in {0,1,2,3}.\n",
        "    \"\"\"\n",
        "    orig_h, orig_w = flair_slice.shape\n",
        "\n",
        "    # Resize to 128x128 (or your model input)\n",
        "    flair_resized = cv2.resize(flair_slice, (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "    t1ce_resized  = cv2.resize(t1ce_slice,  (MODEL_IMG_SIZE, MODEL_IMG_SIZE))\n",
        "\n",
        "    # Stack => shape (1,2,128,128), then normalize\n",
        "    X = np.stack([flair_resized, t1ce_resized], axis=0)[np.newaxis, ...]\n",
        "    max_val = X.max()\n",
        "    if max_val > 0:\n",
        "        X /= max_val\n",
        "\n",
        "    # Convert to tensor\n",
        "    X_tensor = torch.from_numpy(X).float().to(device)  # (1,2,128,128)\n",
        "\n",
        "    # Inference\n",
        "    logits = model(X_tensor)            # (1,4,128,128)\n",
        "    probs  = F.softmax(logits, dim=1)   # (1,4,128,128)\n",
        "    pred   = torch.argmax(probs, dim=1) # (1,128,128)\n",
        "\n",
        "    # Upscale to original slice size\n",
        "    pred_up = cv2.resize(pred[0].cpu().numpy().astype(np.uint8),\n",
        "                         (orig_w, orig_h),\n",
        "                         interpolation=cv2.INTER_NEAREST)\n",
        "    return pred_up\n",
        "\n",
        "###############################################################################\n",
        "# 5) MAIN VISUALIZATION\n",
        "###############################################################################\n",
        "def visualize_case(case=\"355\", slice_idx=60):\n",
        "    \"\"\"\n",
        "    1) Load Flair, T1CE, seg at original resolution.\n",
        "    2) Predict the slice -> discrete mask in {0,1,2,3}.\n",
        "    3) Overlay multi-color mask with alpha for tumor classes.\n",
        "    4) Also show separate class overlays.\n",
        "    \"\"\"\n",
        "    base_path = r\"C:\\Users\\Melisa\\.cache\\kagglehub\\datasets\\awsaf49\\brats20-dataset-training-validation\\versions\\1\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\"\n",
        "    case_path = os.path.join(base_path, f\"BraTS20_Training_{case}\")\n",
        "\n",
        "    # Load volumes\n",
        "    flair_path = os.path.join(case_path, f\"BraTS20_Training_{case}_flair.nii\")\n",
        "    t1ce_path  = os.path.join(case_path, f\"BraTS20_Training_{case}_t1ce.nii\")\n",
        "    seg_path   = os.path.join(case_path, f\"BraTS20_Training_{case}_seg.nii\")\n",
        "\n",
        "    flair_vol = nib.load(flair_path).get_fdata()\n",
        "    t1ce_vol  = nib.load(t1ce_path).get_fdata()\n",
        "    seg_vol   = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    # Convert label 4 -> 3 if present\n",
        "    seg_vol[seg_vol == 4] = 3\n",
        "\n",
        "    # Select the slice (with offset)\n",
        "    z = slice_idx + VOLUME_START_AT\n",
        "    flair_slice = flair_vol[:, :, z]\n",
        "    t1ce_slice  = t1ce_vol[:, :, z]\n",
        "    seg_slice   = seg_vol[:, :, z]\n",
        "\n",
        "    # Predict\n",
        "    pred_mask = predict_slice(flair_slice, t1ce_slice)\n",
        "\n",
        "    # Convert to multi-color overlay\n",
        "    seg_rgb = colorize_segmentation(pred_mask)\n",
        "    alpha_map = make_alpha_map(pred_mask, alpha_value=0.4)\n",
        "\n",
        "    # Convert flair to 3 channels\n",
        "    flair_3ch = cv2.cvtColor(flair_slice.astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    # Build separate binary masks\n",
        "    core_mask       = (pred_mask == 1).astype(np.uint8)*255\n",
        "    edema_mask      = (pred_mask == 2).astype(np.uint8)*255\n",
        "    enhancing_mask  = (pred_mask == 3).astype(np.uint8)*255\n",
        "\n",
        "    # PLOT\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1, 6, figsize=(18, 50))\n",
        "\n",
        "    # 0) Original Flair\n",
        "    axarr[0].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[0].set_title(\"Original image flair\")\n",
        "    axarr[0].axis('off')\n",
        "\n",
        "    # 1) Ground Truth\n",
        "    axarr[1].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[1].imshow(seg_slice, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[1].set_title(\"Ground truth\")\n",
        "    axarr[1].axis('off')\n",
        "\n",
        "    # 2) All classes predicted (multi-color overlay)\n",
        "    axarr[2].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[2].imshow(seg_rgb, alpha=alpha_map)  # black=0 => transparent\n",
        "    axarr[2].set_title(\"all classes predicted\")\n",
        "    axarr[2].axis('off')\n",
        "\n",
        "    # 3) Necrotic/Core predicted\n",
        "    axarr[3].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[3].imshow(core_mask, cmap=\"Reds\", alpha=0.4)\n",
        "    axarr[3].set_title(\"NECROTIC/CORE predicted\")\n",
        "    axarr[3].axis('off')\n",
        "\n",
        "    # 4) Edema predicted\n",
        "    axarr[4].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[4].imshow(edema_mask, cmap=\"Greens\", alpha=0.4)\n",
        "    axarr[4].set_title(\"EDEMA predicted\")\n",
        "    axarr[4].axis('off')\n",
        "\n",
        "    # 5) Enhancing predicted\n",
        "    axarr[5].imshow(flair_slice, cmap=\"gray\")\n",
        "    axarr[5].imshow(enhancing_mask, cmap=\"Blues\", alpha=0.4)\n",
        "    axarr[5].set_title(\"ENHANCING predicted\")\n",
        "    axarr[5].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "###############################################################################\n",
        "# USAGE EXAMPLE\n",
        "###############################################################################\n",
        "# Once you run this entire cell, you can call:\n",
        "visualize_case(case=\"060\", slice_idx=50)\n",
        "# or some other slice with a prominent tumor to get a multi-color overlay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the volumes from your .nii files\n",
        "flair_vol = nib.load(r\"C:\\Users\\Melisa\\.cache\\kagglehub\\datasets\\awsaf49\\brats20-dataset-training-validation\\versions\\1\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_355\\BraTS20_Training_355_flair.nii\").get_fdata()  # e.g. shape (240, 240, 155)\n",
        "ce_vol    = nib.load(r\"C:\\Users\\Melisa\\.cache\\kagglehub\\datasets\\awsaf49\\brats20-dataset-training-validation\\versions\\1\\BraTS2020_TrainingData\\MICCAI_BraTS2020_TrainingData\\BraTS20_Training_355\\BraTS20_Training_355_t1ce.nii\").get_fdata()\n",
        "\n",
        "# Then select the slice\n",
        "slice_idx = 60\n",
        "flair_slice = flair_vol[:, :, slice_idx]\n",
        "ce_slice    = ce_vol[:, :, slice_idx]\n",
        "\n",
        "# Now you can predict\n",
        "pred_slice = predict_slice(flair_slice, ce_slice)\n",
        "pred_slice = predict_slice(flair_slice, ce_slice)\n",
        "unique_vals, counts = np.unique(pred_slice, return_counts=True)\n",
        "print(dict(zip(unique_vals, counts)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cr5Gs8SGGbTd"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[0][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6kIdqaAGe0P"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[10][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yO0XGuqGeqH"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[8][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_maFoFVUGeg3"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[3][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86bTM_PmGeYh"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[9][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEFR3__KGeNj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[11][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru3vAVquGeE4"
      },
      "outputs": [],
      "source": [
        "showPredictsById(case=test_ids[6][-3:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vixPJQVKq12z"
      },
      "outputs": [],
      "source": [
        "def predict_segmentation(sample_path):\n",
        "    # Load NIfTI (.nii) files of the sample (patient)\n",
        "    t1ce_path = sample_path + '_t1ce.nii'\n",
        "    flair_path = sample_path + '_flair.nii'\n",
        "\n",
        "    # Extract the data from these paths\n",
        "    t1ce = nib.load(t1ce_path).get_fdata()\n",
        "    flair = nib.load(flair_path).get_fdata()\n",
        "\n",
        "    # Create an empty array\n",
        "    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "    # Perform the same operations as our DataGenerator, to keep the same input shape\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "        X[j,:,:,1] = cv2.resize(t1ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "    # Send our images to the CNN model and return predicted segmentation\n",
        "    return model.predict(X/np.max(X), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEkYy-ZVq1rb"
      },
      "outputs": [],
      "source": [
        "def show_predicted_segmentations(samples_list, slice_to_plot, cmap, norm):\n",
        "    # Choose a random patient\n",
        "    random_sample = random.choice(samples_list)\n",
        "\n",
        "    # Get path of this patient\n",
        "    random_sample_path = os.path.join(TRAIN_DATASET_PATH, random_sample, random_sample)\n",
        "\n",
        "    # Predict patient's segmentation\n",
        "    predicted_seg = predict_segmentation(random_sample_path)\n",
        "\n",
        "    # Load patient's original segmentation (Ground truth)\n",
        "    seg_path = random_sample_path + '_seg.nii'\n",
        "    seg = nib.load(seg_path).get_fdata()\n",
        "\n",
        "    # Resize original segmentation to the same dimensions of the predictions. (Add VOLUME_START_AT because original segmentation contains 155 slices vs only 75 for our prediction)\n",
        "    seg=cv2.resize(seg[:,:,slice_to_plot+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n",
        "\n",
        "    # Differentiate segmentations by their labels\n",
        "    all = predicted_seg[slice_to_plot,:,:,1:4] # Deletion of class 0 (Keep only Core + Edema + Enhancing classes)\n",
        "    zero = predicted_seg[slice_to_plot,:,:,0] # Isolation of class 0, Background (kind of useless, it is the opposite of the \"all\")\n",
        "    first = predicted_seg[slice_to_plot,:,:,1] # Isolation of class 1, Core\n",
        "    second = predicted_seg[slice_to_plot,:,:,2] # Isolation of class 2, Edema\n",
        "    third = predicted_seg[slice_to_plot,:,:,3] # Isolation of class 3, Enhancing\n",
        "\n",
        "    # Plot Original segmentation & predicted segmentation\n",
        "    print(\"Patient number: \", random_sample)\n",
        "    fig, axstest = plt.subplots(1, 6, figsize=(25, 20))\n",
        "\n",
        "    # Original segmentation\n",
        "    axstest[0].imshow(seg, cmap, norm)\n",
        "    axstest[0].set_title('Original Segmentation')\n",
        "\n",
        "    # Layers 1, 2, 3\n",
        "    axstest[1].imshow(all, cmap, norm)\n",
        "    axstest[1].set_title('Predicted Segmentation - all classes')\n",
        "\n",
        "    # Layer 0\n",
        "    axstest[2].imshow(zero)\n",
        "    axstest[2].set_title('Predicted Segmentation - Not Tumor')\n",
        "\n",
        "    # Layer 1\n",
        "    axstest[3].imshow(first)\n",
        "    axstest[3].set_title('Predicted Segmentation - Necrotic/Core')\n",
        "\n",
        "    # Layer 2\n",
        "    axstest[4].imshow(second)\n",
        "    axstest[4].set_title('Predicted Segmentation - Edema')\n",
        "\n",
        "    # Layer 3\n",
        "    axstest[5].imshow(third)\n",
        "    axstest[5].set_title('Predicted Segmentation - Enhancing')\n",
        "\n",
        "    # Add space between subplots\n",
        "    plt.subplots_adjust(wspace=0.8)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_zxQ3O0q8Ms"
      },
      "outputs": [],
      "source": [
        "show_predicted_segmentations(test_ids, 70, cmap, norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jpi6X3Rq8JN"
      },
      "outputs": [],
      "source": [
        "show_predicted_segmentations(test_ids, 70, cmap, norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ReeXAgjFJ5w"
      },
      "outputs": [],
      "source": [
        "show_predicted_segmentations(test_ids, 65, cmap, norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjoZgqd9ikyi"
      },
      "outputs": [],
      "source": [
        "case = test_ids[3][-3:]\n",
        "path = f\"/content/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/BraTS20_Training_{case}\"\n",
        "gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n",
        "p = predictByPath(path,case)\n",
        "\n",
        "core = p[:,:,:,1]\n",
        "edema= p[:,:,:,2]\n",
        "enhancing = p[:,:,:,3]\n",
        "\n",
        "i=40 # slice at\n",
        "eval_class = 2 #     0 : 'NOT tumor',  1 : 'ENHANCING',    2 : 'CORE',    3 : 'WHOLE'\n",
        "\n",
        "gt[gt != eval_class] = 1 # use only one class for per class evaluation\n",
        "\n",
        "resized_gt = cv2.resize(gt[:,:,i+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "plt.figure()\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(resized_gt, cmap=\"gray\")\n",
        "axarr[0].title.set_text('ground truth')\n",
        "axarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\n",
        "axarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWWCPM35dOSH"
      },
      "source": [
        "# 13. Evaluation\n",
        "\n",
        "Let's call the `evaluate()` function to evaluate the performance of our model on our test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ5NpZ3BlwGb"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing])\n",
        "\n",
        "results = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\n",
        "\n",
        "descriptions = [\"Loss\", \"Accuracy\", \"MeanIOU\", \"Dice coefficient\", \"Precision\", \"Sensitivity\", \"Specificity\", \"Dice coef Necrotic\", \"Dice coef Edema\", \"Dice coef Enhancing\"]\n",
        "\n",
        "# Combine results list and descriptions list\n",
        "results_list = zip(results, descriptions)\n",
        "\n",
        "# Display each metric with its description\n",
        "print(\"\\nModel evaluation on the test set:\")\n",
        "print(\"==================================\")\n",
        "for i, (metric, description) in enumerate(results_list):\n",
        "    print(f\"{description} : {round(metric, 4)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04ac9ac22db044eda67118d5123efff4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f4502b973e94f83a514357d4af8d7ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "116370f303ad465faa2fc711bb4ab9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04ac9ac22db044eda67118d5123efff4",
            "placeholder": "​",
            "style": "IPY_MODEL_641af6f7b5014471ab6f24ec3a3bf925",
            "value": "  0%"
          }
        },
        "17e7802278dc4ef3b24f3987c8ae42a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25b90c948c8d448b80433e248852f087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "501a85dcf0eb4d8fbbddc5c0a76d2b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6973166624254a29a88a39bb4872f85c",
              "IPY_MODEL_be3b762b7c504a39b73c4e19d6611789",
              "IPY_MODEL_79ae90365e32411288d308da5c495506"
            ],
            "layout": "IPY_MODEL_8fed9bd56b654e8282e2051abd63e269"
          }
        },
        "5ef7f051eb4041b99e601bb8b0c8eaaf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641af6f7b5014471ab6f24ec3a3bf925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6973166624254a29a88a39bb4872f85c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_927bba9ac70e49d4b5370c4956962bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_0f4502b973e94f83a514357d4af8d7ee",
            "value": " 70%"
          }
        },
        "7735b0d08bc744d192922c5f3d04d4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f001f64c5ca46cbb72ab0b01f79deac",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a0b8e4fe85bf4b4bab85a476c9d5bee8",
            "value": 0
          }
        },
        "79ae90365e32411288d308da5c495506": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37c623946084b83a668c3371d9837a8",
            "placeholder": "​",
            "style": "IPY_MODEL_25b90c948c8d448b80433e248852f087",
            "value": " 174/250 [03:33&lt;00:54, 1.39batch/s, accuracy=0.98, dice_coef=0.381, dice_coef_edema=0.421, dice_coef_enhancing=0.298, dice_coef_necrotic=0.162, loss=0.148, mean_io_u_1=0.436, precision=0.982, sensitivity=0.977, specificity=0.994]"
          }
        },
        "7f001f64c5ca46cbb72ab0b01f79deac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896931e949bc465586f42f1d6de2b7ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ef7f051eb4041b99e601bb8b0c8eaaf",
            "placeholder": "​",
            "style": "IPY_MODEL_e02e5e2aabc24dd88d457d4edfad5d79",
            "value": " 0/10 [00:00&lt;?, ?epoch/s]"
          }
        },
        "8fed9bd56b654e8282e2051abd63e269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "927bba9ac70e49d4b5370c4956962bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "952fe96c7feb46a6aa66b925cd403fdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0b8e4fe85bf4b4bab85a476c9d5bee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a14c5c1369cd4c57a8df012ad194018f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be3b762b7c504a39b73c4e19d6611789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a14c5c1369cd4c57a8df012ad194018f",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17e7802278dc4ef3b24f3987c8ae42a5",
            "value": 174
          }
        },
        "c38b3a41f4b34a26a1ed1b940656bd28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_116370f303ad465faa2fc711bb4ab9d0",
              "IPY_MODEL_7735b0d08bc744d192922c5f3d04d4c3",
              "IPY_MODEL_896931e949bc465586f42f1d6de2b7ce"
            ],
            "layout": "IPY_MODEL_952fe96c7feb46a6aa66b925cd403fdf"
          }
        },
        "d37c623946084b83a668c3371d9837a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02e5e2aabc24dd88d457d4edfad5d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
